{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leash Bio\n",
    "\n",
    "- RDkit記述子とfingerprintを入れる\n",
    "- 藤田さんのアーキテクチャ\n",
    "\n",
    "## ref\n",
    "- https://www.kaggle.com/code/yyyu54/pytorch-version-belka-1dcnn-starter-with-all-data\n",
    "- https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = '018'\n",
    "DEBUG = True\n",
    "data_ratio = 1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdkit\n",
    "# !pip install mordred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# import polars as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "# seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from funcs.utils import find_latest_ckpt_path, del_old_ckpt_path\n",
    "from funcs.calc_descriptor import (calc_ecfp4_descriptors,\n",
    "                                   calc_avalonfp_descriptors,\n",
    "                                   calc_fcfp4_descriptors, \n",
    "                                   calc_rdkit_descriptors)\n",
    "from funcs.tokenize import tokenize_smiles\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPU(s)\n",
      "pytorch: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def is_kaggle_kernel():\n",
    "    return os.path.exists('/kaggle/working')\n",
    "\n",
    "if is_kaggle_kernel():\n",
    "\n",
    "    BASE_DIR = Path(\"/kaggle\")\n",
    "    DATA_DIR = BASE_DIR / \"input\"\n",
    "    OUTPUT_DIR = BASE_DIR / \"working\"\n",
    "    print('on kaggle notebook')\n",
    "\n",
    "else:\n",
    "    BASE_DIR = Path(os.getcwd()) / './../'\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    OUTPUT_DIR = BASE_DIR / f\"output/exp{exp_no}\"\n",
    "    \n",
    "# set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():    \n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "print('pytorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    SEED = 2024\n",
    "    \n",
    "    PREPROCESS = False\n",
    "    EPOCHS = 20 #20\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 16\n",
    "    \n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "    NUM_FOLDS = 5    \n",
    "    USE_NUM_FOLD = 1\n",
    "    \n",
    "class paths:    \n",
    "    DATA_DIR = DATA_DIR\n",
    "    OUTPUT_DIR = OUTPUT_DIR\n",
    "    MODEL_WEIGHTS_DIR = OUTPUT_DIR / f\"bio-models-exp{exp_no}\"\n",
    "    \n",
    "    SHRUNKEN_DATA_DIR = DATA_DIR / \"shrunken-train-set\"\n",
    "\n",
    "    TRAIN_PATH = SHRUNKEN_DATA_DIR / \"train_fold.parquet\"\n",
    "    TEST_PATH = SHRUNKEN_DATA_DIR / \"test_fold.parquet\"\n",
    "    \n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    }
   ],
   "source": [
    "print('fix seed')\n",
    "\n",
    "def my_seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "# seed_everything(config.SEED, workers=True)\n",
    "my_seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loda Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_cols = ['buildingblock1_smiles', 'buildingblock2_smiles','buildingblock3_smiles', 'bb1_scaffold_idx', 'fold']\n",
    "TARGETS = ['binds_BRD4', 'binds_HSA','binds_sEH']\n",
    "\n",
    "df_train = pd.read_parquet(paths.TRAIN_PATH, columns=bb_cols + TARGETS)\n",
    "    \n",
    "if DEBUG:\n",
    "    df_train = df_train.sample(100000).reset_index(drop=True)\n",
    "else:\n",
    "    len_train = int(len(df_train)*data_ratio)\n",
    "    df_train = df_train.sample(len_train).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building block smiles\n",
    "# NOTE: trainとtestのindexとsmilesは一致していないっぽい\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_1.p', 'rb') as file:\n",
    "    train_dicts_bb1 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_2.p', 'rb') as file:\n",
    "    train_dicts_bb2 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_3.p', 'rb') as file:\n",
    "    train_dicts_bb3 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_scaffold_dict_reverse_1.p', 'rb') as file:\n",
    "    train_dicts_bb1_scf = pickle.load(file)\n",
    "\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_1_test.p', 'rb') as file:\n",
    "    test_dicts_bb1 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_2_test.p', 'rb') as file:\n",
    "    test_dicts_bb2 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_3_test.p', 'rb') as file:\n",
    "    test_dicts_bb3= pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_scaffold_dict_reverse_1.p', 'rb') as file:\n",
    "    test_dicts_bb1_scf = pickle.load(file)\n",
    "    \n",
    "# bb1のidxをscaffoldのidxに変換するdict\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_idx_to_scaffold_idx_dict_1.p', mode='rb') as file:\n",
    "    test_bb1idx2scfidx= pickle.load(file)\n",
    "\n",
    "test_dicts_bb1_reverse = {val:key for key, val in test_dicts_bb1.items()}\n",
    "test_dicts_bb2_reverse = {val:key for key, val in test_dicts_bb2.items()}\n",
    "test_dicts_bb3_reverse = {val:key for key, val in test_dicts_bb3.items()}\n",
    "test_dicts_bb1_scaffold_reverse = {val:key for key, val in test_dicts_bb1_scf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(paths.DATA_DIR / 'test.parquet')\n",
    "df_test.drop(['molecule_smiles'], axis=1, inplace=True)\n",
    "\n",
    "df_test['buildingblock1_smiles'] = df_test['buildingblock1_smiles'].map(test_dicts_bb1_reverse)\n",
    "df_test['buildingblock2_smiles'] = df_test['buildingblock2_smiles'].map(test_dicts_bb2_reverse)\n",
    "df_test['buildingblock3_smiles'] = df_test['buildingblock3_smiles'].map(test_dicts_bb3_reverse)\n",
    "\n",
    "df_test['bb1_scaffold_idx'] = df_test['buildingblock1_smiles'].map(test_bb1idx2scfidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdkit descriptors\n",
    "df_train_bb1_rdkit = calc_rdkit_descriptors(train_dicts_bb1)\n",
    "df_train_bb2_rdkit = calc_rdkit_descriptors(train_dicts_bb2)\n",
    "df_train_bb3_rdkit = calc_rdkit_descriptors(train_dicts_bb3)\n",
    "df_train_bb1_scf_rdkit = calc_rdkit_descriptors(train_dicts_bb1_scf)\n",
    "\n",
    "df_test_bb1_rdkit = calc_rdkit_descriptors(test_dicts_bb1)\n",
    "df_test_bb2_rdkit = calc_rdkit_descriptors(test_dicts_bb2)\n",
    "df_test_bb3_rdkit = calc_rdkit_descriptors(test_dicts_bb3)\n",
    "df_test_bb1_scf_rdkit = calc_rdkit_descriptors(test_dicts_bb1_scf)\n",
    "\n",
    "\n",
    "df_train_bb1_ecfp4 = calc_avalonfp_descriptors(train_dicts_bb1)\n",
    "df_train_bb2_ecfp4 = calc_avalonfp_descriptors(train_dicts_bb2)\n",
    "df_train_bb3_ecfp4 = calc_avalonfp_descriptors(train_dicts_bb3)\n",
    "df_train_bb1_scf_ecfp4 = calc_avalonfp_descriptors(train_dicts_bb1_scf)\n",
    "\n",
    "df_test_bb1_ecfp4 = calc_avalonfp_descriptors(test_dicts_bb1)\n",
    "df_test_bb2_ecfp4 = calc_avalonfp_descriptors(test_dicts_bb2)\n",
    "df_test_bb3_ecfp4 = calc_avalonfp_descriptors(test_dicts_bb3)\n",
    "df_test_bb1_scf_ecfp4 = calc_avalonfp_descriptors(test_dicts_bb1_scf)\n",
    "\n",
    "# input_len = df_train_bb1.shape[1]\n",
    "# print('input_len:', input_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_std0(df_list):\n",
    "    # 標準偏差が0の列を削除\n",
    "    df_all = pd.concat(df_list,axis=0)\n",
    "    df_all.drop_duplicates(inplace=True)\n",
    "    df_all = df_all.loc[:, df_all.std() != 0]\n",
    "    \n",
    "    standardized_df_list = []\n",
    "    for df_temp in df_list:\n",
    "        df_temp = df_temp.loc[:, df_all.columns]\n",
    "        standardized_df_list.append(df_temp)\n",
    "        \n",
    "    return standardized_df_list\n",
    "\n",
    "\n",
    "def standardization(df_list):\n",
    "    # 複数のdfをまとめて標準化\n",
    "    df_all = pd.concat(df_list,axis=0)\n",
    "    df_all.drop_duplicates(inplace=True)\n",
    "    df_all.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    df_all_array = scaler.fit_transform(df_all)\n",
    "    \n",
    "    # 全てnanの列を検出しておく\n",
    "    nan_columns = np.all(np.isnan(df_all_array), axis=0)\n",
    "    del_cols = df_all.columns[nan_columns]\n",
    "\n",
    "    standardized_df_list = []\n",
    "    for df_temp in df_list:\n",
    "        df_temp = df_temp.loc[:, df_all.columns]\n",
    "        df_temp_std = pd.DataFrame(scaler.transform(df_temp), \n",
    "                                index=df_temp.index, \n",
    "                                columns=df_temp.columns)\n",
    "        df_temp_std.drop(columns=del_cols, inplace=True)\n",
    "        standardized_df_list.append(df_temp_std)\n",
    "        \n",
    "    standardized_df_list = remove_std0(standardized_df_list)\n",
    "        \n",
    "    return standardized_df_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rdkit記述子をまとめて標準化\n",
    "df_list_rdkit = [\n",
    "            df_train_bb1_rdkit,df_train_bb2_rdkit, df_train_bb3_rdkit, df_train_bb1_scf_rdkit,\n",
    "            df_test_bb1_rdkit,df_test_bb2_rdkit,df_test_bb3_rdkit,df_test_bb1_scf_rdkit\n",
    "            ]\n",
    "df_train_bb1_rdkit,df_train_bb2_rdkit, df_train_bb3_rdkit, df_train_bb1_scf_rdkit, \\\n",
    "    df_test_bb1_rdkit,df_test_bb2_rdkit,df_test_bb3_rdkit,df_test_bb1_scf_rdkit \\\n",
    "        = standardization(df_list_rdkit)\n",
    "        \n",
    "# ECFP4記述子をまとめて標準化\n",
    "df_list_ecfp4 = [\n",
    "            df_train_bb1_ecfp4,df_train_bb2_ecfp4, df_train_bb3_ecfp4, df_train_bb1_scf_ecfp4,\n",
    "            df_test_bb1_ecfp4,df_test_bb2_ecfp4,df_test_bb3_ecfp4,df_test_bb1_scf_ecfp4\n",
    "            ]\n",
    "df_train_bb1_ecfp4,df_train_bb2_ecfp4, df_train_bb3_ecfp4, df_train_bb1_scf_ecfp4,\\\n",
    "            df_test_bb1_ecfp4,df_test_bb2_ecfp4,df_test_bb3_ecfp4,df_test_bb1_scf_ecfp4 \\\n",
    "                = remove_std0(df_list_ecfp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 512\n"
     ]
    }
   ],
   "source": [
    "len_rdkit = df_train_bb1_rdkit.shape[1]\n",
    "len_ecfp4 = df_train_bb1_ecfp4.shape[1]\n",
    "print(len_rdkit, len_ecfp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset & DataModule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        df_bb1_1: pd.DataFrame,\n",
    "        df_bb2_1: pd.DataFrame,\n",
    "        df_bb3_1: pd.DataFrame,\n",
    "        df_bb1_scf_1: pd.DataFrame,\n",
    "        df_bb1_2: pd.DataFrame,\n",
    "        df_bb2_2: pd.DataFrame,\n",
    "        df_bb3_2: pd.DataFrame,\n",
    "        df_bb1_scf_2: pd.DataFrame,\n",
    "        mode = 'train'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert mode in ['train', 'valid', 'test']\n",
    "        self.mode = mode\n",
    "        \n",
    "        meta_cols = [\"buildingblock1_smiles\", \"buildingblock2_smiles\", \"buildingblock3_smiles\", \"bb1_scaffold_idx\"]\n",
    "        if (self.mode == 'train') or (self.mode == 'valid'):\n",
    "            meta_cols += TARGETS\n",
    "            \n",
    "        self.df = df[meta_cols].values\n",
    "        self.bb1_1 = df_bb1_1.values\n",
    "        self.bb2_1 = df_bb2_1.values\n",
    "        self.bb3_1 = df_bb3_1.values\n",
    "        self.bb1_scf_1 = df_bb1_scf_1.values\n",
    "        self.bb1_2 = df_bb1_2.values\n",
    "        self.bb2_2 = df_bb2_2.values\n",
    "        self.bb3_2 = df_bb3_2.values\n",
    "        self.bb1_scf_2 = df_bb1_scf_2.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.df[index, :]\n",
    "\n",
    "        bb1_1 = self.bb1_1[row[0], :]\n",
    "        bb2_1 = self.bb2_1[row[1], :]\n",
    "        bb3_1 = self.bb3_1[row[2], :]\n",
    "        bb1_scf_1 = self.bb1_scf_1[row[3], :]\n",
    "        bb1_2 = self.bb1_2[row[0], :]\n",
    "        bb2_2 = self.bb2_2[row[1], :]\n",
    "        bb3_2 = self.bb3_2[row[2], :]\n",
    "        bb1_scf_2 = self.bb1_scf_2[row[3], :]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            bb2_1, bb3_1, bb2_2, bb3_2 = self.augment(bb2_1, bb3_1, bb2_2, bb3_2)\n",
    "        \n",
    "        X = np.concatenate([bb1_1, bb2_1, bb3_1, bb1_scf_1,\n",
    "                             bb1_2, bb2_2, bb3_2, bb1_scf_2])\n",
    "        \n",
    "        if (self.mode == 'train') or (self.mode == 'valid'):\n",
    "            y = row[-3:]\n",
    "        else:\n",
    "            y = np.zeros(3)\n",
    "        \n",
    "        output = {\n",
    "            'X': torch.tensor(X, dtype=torch.float32),\n",
    "            'y': torch.tensor(y, dtype=torch.float16)\n",
    "        }        \n",
    "        return output\n",
    "    \n",
    "    def augment(self, bb2_1, bb3_1, bb2_2, bb3_2):\n",
    "        \"\"\"0.5の確率でx2とx3を入れ替えるaugmentation\"\"\"\n",
    "        if np.random.rand() < 0.5:\n",
    "            bb2_1, bb3_1 = bb3_1, bb2_1\n",
    "            bb2_2, bb3_2 = bb3_2, bb2_2\n",
    "        return bb2_1, bb3_1, bb2_2, bb3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9568])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Check Dataset\n",
    "if DEBUG:\n",
    "    dataset = BioDataset(df_train, \n",
    "                         df_train_bb1_rdkit,df_train_bb2_rdkit, df_train_bb3_rdkit, df_train_bb1_scf_rdkit,\n",
    "                         df_train_bb1_ecfp4,df_train_bb2_ecfp4, df_train_bb3_ecfp4, df_train_bb1_scf_ecfp4,\n",
    "                         mode='valid')\n",
    "    X = dataset[0]['X']\n",
    "    y = dataset[0]['y']\n",
    "    print(X.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning data module\n",
    "class BioDataModule(LightningDataModule):\n",
    "    def __init__(self, df_train, fold_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = df_train[df_train['fold'] != fold_id]\n",
    "        self.valid_df = df_train[df_train['fold'] == fold_id]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = BioDataset(self.train_df, \n",
    "                                   df_train_bb1_rdkit,df_train_bb2_rdkit, df_train_bb3_rdkit, df_train_bb1_scf_rdkit,\n",
    "                                    df_train_bb1_ecfp4,df_train_bb2_ecfp4, df_train_bb3_ecfp4, df_train_bb1_scf_ecfp4,\n",
    "                                   mode='train')\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=config.BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=config.NUM_WORKERS,\n",
    "                                pin_memory=True,\n",
    "                                persistent_workers=True,\n",
    "                                drop_last=True,\n",
    "                            )\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = BioDataset(self.valid_df, \n",
    "                                     df_train_bb1_rdkit,df_train_bb2_rdkit, df_train_bb3_rdkit, df_train_bb1_scf_rdkit,\n",
    "                                    df_train_bb1_ecfp4,df_train_bb2_ecfp4, df_train_bb3_ecfp4, df_train_bb1_scf_ecfp4, \n",
    "                                   mode='valid')\n",
    "        valid_dataloader = torch.utils.data.DataLoader(\n",
    "                                            valid_dataset,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config.NUM_WORKERS,\n",
    "                                            pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            drop_last=False,\n",
    "                                        )\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_len1,\n",
    "                 input_len2,\n",
    "                 output_dim=3):\n",
    "        super(BioModel, self).__init__()\n",
    "        \n",
    "        self.input_len1 = input_len1\n",
    "        self.input_len2 = input_len2\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # それぞれの記述子のFC（desc1）\n",
    "        self.feature_extractor_bb1_desc1 = nn.Sequential(\n",
    "            nn.Linear(self.input_len1, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_bb23_desc1 = nn.Sequential(\n",
    "            nn.Linear(self.input_len1, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_bb1scf_desc1 = nn.Sequential(\n",
    "            nn.Linear(self.input_len1, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "         # それぞれの記述子のFC（desc2）\n",
    "        self.feature_extractor_bb1_desc2 = nn.Sequential(\n",
    "            nn.Linear(self.input_len2, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_bb23_desc2 = nn.Sequential(\n",
    "            nn.Linear(self.input_len2, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_bb1scf_desc2 = nn.Sequential(\n",
    "            nn.Linear(self.input_len2, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # それぞれのBBのFC\n",
    "        self.feature_extractor_bb1 = nn.Sequential(\n",
    "            nn.Linear(128*2, 324),\n",
    "            nn.BatchNorm1d(324),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_bb23 = nn.Sequential(\n",
    "            nn.Linear(128*2, 324),\n",
    "            nn.BatchNorm1d(324),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_bbscf = nn.Sequential(\n",
    "            nn.Linear(128*2, 324),\n",
    "            nn.BatchNorm1d(324),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(324*4, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "        )  \n",
    "\n",
    "    def forward(self, x):\n",
    "        desc1 = x[:, :self.input_len1*4]\n",
    "        desc2 = x[:, self.input_len1*4:]\n",
    "        \n",
    "        # 各BB, 各記述子のFC\n",
    "        bb1_1 = self.feature_extractor_bb1_desc1(desc1[:, :self.input_len1])\n",
    "        bb2_1 = self.feature_extractor_bb23_desc1(desc1[:, self.input_len1:self.input_len1*2])\n",
    "        bb3_1 = self.feature_extractor_bb23_desc1(desc1[:, self.input_len1*2:self.input_len1*3])\n",
    "        bb1_scf_1 = self.feature_extractor_bb1scf_desc1(desc1[:, self.input_len1*3:])\n",
    "        bb1_2 = self.feature_extractor_bb1_desc2(desc2[:, :self.input_len2])\n",
    "        bb2_2 = self.feature_extractor_bb23_desc2(desc2[:, self.input_len2:self.input_len2*2])\n",
    "        bb3_2 = self.feature_extractor_bb23_desc2(desc2[:, self.input_len2*2:self.input_len2*3])\n",
    "        bb1_scf_2 = self.feature_extractor_bb1scf_desc2(desc2[:, self.input_len2*3:])\n",
    "        \n",
    "        bb1 = torch.cat([bb1_1, bb1_2], dim=1)\n",
    "        bb2 = torch.cat([bb2_1, bb2_2], dim=1)\n",
    "        bb3 = torch.cat([bb3_1, bb3_2], dim=1)\n",
    "        bbscf = torch.cat([bb1_scf_1, bb1_scf_2], dim=1)   \n",
    "        \n",
    "        # 各BBのFC\n",
    "        bb1 = self.feature_extractor_bb1(bb1)\n",
    "        bb2 = self.feature_extractor_bb23(bb2)\n",
    "        bb3 = self.feature_extractor_bb23(bb3)\n",
    "        bbscf = self.feature_extractor_bb23(bbscf)\n",
    "        \n",
    "        X = torch.cat([bb1, bb2, bb3, bbscf], dim=1)\n",
    "        \n",
    "        output = self.head(X)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 3030119\n",
      "torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "if DEBUG:\n",
    "    dummy_model = BioModel(input_len1=len_rdkit, input_len2=len_ecfp4)\n",
    "    total_params = sum(p.numel() for p in dummy_model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "    dummy_input = torch.rand((64, (len_rdkit+len_ecfp4)*4), dtype=torch.float32)\n",
    "    output = dummy_model(dummy_input)\n",
    "    print(output.shape)\n",
    "    # print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(y_preds, y_true):\n",
    "    score_BRD4 = APS(y_true[:,0], y_preds[:,0])\n",
    "    score_HSA = APS(y_true[:,1], y_preds[:,1])\n",
    "    score_sEH = APS(y_true[:,2], y_preds[:,2])\n",
    "    score = (score_BRD4 + score_HSA + score_sEH) / 3\n",
    "    \n",
    "    return score_BRD4, score_HSA, score_sEH, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BioModule, self).__init__()\n",
    "       \n",
    "        self.model = BioModel(input_len1=len_rdkit, input_len2=len_ecfp4)\n",
    "        self.validation_step_outputs = []\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pred = self.model(X)\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'valid_loss_epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        train_loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log('train_loss', train_loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "        \n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        \n",
    "        valid_loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log('valid_loss', valid_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "        \n",
    "        self.validation_step_outputs.append({\"valid_loss\":valid_loss, \"preds\":preds, \"targets\":y})\n",
    "        \n",
    "        return valid_loss\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def calc_score(self, y_preds, y_true):\n",
    "        return calc_score(y_preds, y_true)\n",
    "\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        # 各iterationごとのlossを平均\n",
    "        avg_loss = torch.stack([x['valid_loss'] for x in outputs]).mean()\n",
    "        self.log(\"valid_loss_epoch\", avg_loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        # scoreを計算\n",
    "        y_preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n",
    "        y_true = torch.cat([x['targets'] for x in outputs]).detach().cpu().numpy()\n",
    "        \n",
    "        score = self.calc_score(y_preds, y_true)[-1]\n",
    "        self.log(\"valid_score\", score, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "        \n",
    "        return {'valid_loss_epoch': avg_loss, \"valid_score\":score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, df, \n",
    "                       df_bb1_1, df_bb2_1, df_bb3_1, df_bb1_scf_1, \n",
    "                       df_bb1_2, df_bb2_2, df_bb3_2, df_bb1_scf_2, \n",
    "                       mode):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = BioDataset(df, \n",
    "                          df_bb1_1, df_bb2_1, df_bb3_1, df_bb1_scf_1, \n",
    "                       df_bb1_2, df_bb2_2, df_bb3_2, df_bb1_scf_2, \n",
    "                         mode=mode)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                                        dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=config.NUM_WORKERS,\n",
    "                                        pin_memory=True,\n",
    "                                        persistent_workers=True,\n",
    "                                        drop_last=False,\n",
    "                                    )\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['X'].to(device)\n",
    "            logits = model(inputs)\n",
    "            preds = torch.sigmoid(logits)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, df, infer=False):\n",
    "    print(f\"======== Running training for fold {fold_id} =============\")\n",
    "    \n",
    "    # == init data module and model ==\n",
    "    model = BioModule()\n",
    "    datamodule = BioDataModule(df, fold_id)\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                                        monitor='valid_score',\n",
    "#                                             monitor='valid_loss_epoch',\n",
    "                                          dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='max'\n",
    "                                          )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='valid_score',\n",
    "#         monitor=\"valid_loss_epoch\", \n",
    "        mode=\"max\", \n",
    "        patience=5,\n",
    "        verbose=True\n",
    "        )\n",
    "    callbacks_to_use = [checkpoint_callback,\n",
    "                        early_stop_callback,\n",
    "                        RichModelSummary(),\n",
    "                        RichProgressBar(),\n",
    "                       ]\n",
    "\n",
    "    # == init trainer ==\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        callbacks=callbacks_to_use,\n",
    "        accelerator=device,\n",
    "        devices=-1,  # 全ての利用可能なGPUを使用\n",
    "        deterministic=False,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}'),\n",
    "    )\n",
    "    \n",
    "    if not infer:\n",
    "        # == Training ==\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        weights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "    else:\n",
    "        ckpt_path = find_latest_ckpt_path(fold_id, paths.MODEL_WEIGHTS_DIR) \n",
    "        weights = torch.load(ckpt_path)['state_dict']\n",
    "        \n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    valid_df = datamodule.valid_df\n",
    "    \n",
    "    preds_oof = predict_in_batches(model, valid_df, \n",
    "                                     df_train_bb1_rdkit,df_train_bb2_rdkit, df_train_bb3_rdkit, df_train_bb1_scf_rdkit,\n",
    "                                    df_train_bb1_ecfp4,df_train_bb2_ecfp4, df_train_bb3_ecfp4, df_train_bb1_scf_ecfp4,\n",
    "                                   mode='valid')\n",
    "    y_oof = valid_df[TARGETS].values\n",
    "    \n",
    "    score_BRD4, score_HSA, score_sEH, score = calc_score(preds_oof, y_oof)\n",
    "    \n",
    "    valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
    "    \n",
    "    print(f'fold:{fold_id} | CV score = {score}')\n",
    "    \n",
    "    df_test_temp = df_test.drop(['id'], axis=1)\n",
    "    preds_test = predict_in_batches(model, df_test_temp, \n",
    "                                      df_test_bb1_rdkit,df_test_bb2_rdkit, df_test_bb3_rdkit, df_test_bb1_scf_rdkit,\n",
    "                                    df_test_bb1_ecfp4,df_test_bb2_ecfp4, df_test_bb3_ecfp4, df_test_bb1_scf_ecfp4,\n",
    "                                    mode='test')\n",
    "    \n",
    "    del model, datamodule, trainer, preds_oof, y_oof\n",
    "    gc.collect()\n",
    "    \n",
    "    score_dict = {\n",
    "        'BRD4':score_BRD4,\n",
    "        \"HSA\":score_HSA,\n",
    "        \"sEH\":score_sEH,\n",
    "        \"all\":score\n",
    "    }\n",
    "    \n",
    "    return preds_test, score_dict, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 1/19 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">16/16</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:00 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">49.85it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 train_loss_step: 0.030</span>\n",
       "                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">valid_loss: 0.392 valid_loss_epoch:</span>\n",
       "                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.392 valid_score: 0.010           </span>\n",
       "                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_epoch: 0.234            </span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Validation</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3/8  </span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:00 • 0:00:02</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">4.40it/s </span>                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 1/19 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m16/16\u001b[0m \u001b[38;5;245m0:00:00 • 0:00:00\u001b[0m \u001b[38;5;249m49.85it/s\u001b[0m \u001b[37mv_num: 0.000 train_loss_step: 0.030\u001b[0m\n",
       "                                                                                \u001b[37mvalid_loss: 0.392 valid_loss_epoch:\u001b[0m\n",
       "                                                                                \u001b[37m0.392 valid_score: 0.010           \u001b[0m\n",
       "                                                                                \u001b[37mtrain_loss_epoch: 0.234            \u001b[0m\n",
       "\u001b[37mValidation\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━\u001b[0m\u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m3/8  \u001b[0m \u001b[38;5;245m0:00:00 • 0:00:02\u001b[0m \u001b[38;5;249m4.40it/s \u001b[0m                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pin memory thread exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(score_list_txt))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mNUM_FOLDS):\n\u001b[0;32m---> 22\u001b[0m     preds_test, score_dict, df_oof \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# save score\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     score_list_BRD4\u001b[38;5;241m.\u001b[39mappend(score_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRD4\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[30], line 45\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(fold_id, df, infer)\u001b[0m\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mEPOCHS,\n\u001b[1;32m     35\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks_to_use,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     logger\u001b[38;5;241m=\u001b[39mTensorBoardLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m infer:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# == Training ==\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py:128\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1290\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# while condition is false, i.e., pin_memory_thread died.\u001b[39;00m\n\u001b[0;32m-> 1290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPin memory thread exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Pin memory thread exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# tokenizerの warning対策\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "all_preds = []\n",
    "score_list = []\n",
    "score_list_BRD4 = []\n",
    "score_list_HSA = []\n",
    "score_list_sEH = []\n",
    "\n",
    "def save_list_by_text(score_list, filename):\n",
    "    # ファイルに書き込み\n",
    "    score_list_txt = [str(loss) for loss in score_list]\n",
    "    with open(paths.OUTPUT_DIR / f'{filename}.txt', 'w') as file:\n",
    "        file.write(', '.join(score_list_txt))\n",
    "    \n",
    "\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    \n",
    "    preds_test, score_dict, df_oof = run_training(fold_id, df_train, infer=False)\n",
    "    \n",
    "    # save score\n",
    "    score_list_BRD4.append(score_dict['BRD4'])\n",
    "    score_list_HSA.append(score_dict['HSA'])\n",
    "    score_list_sEH.append(score_dict['sEH'])\n",
    "    score_list.append(score_dict['all'])\n",
    "    \n",
    "    save_list_by_text(score_list, 'cv_all')\n",
    "    save_list_by_text(score_list_BRD4, 'cv_BRD4')\n",
    "    save_list_by_text(score_list_HSA, 'cv_HSA')\n",
    "    save_list_by_text(score_list_sEH, 'cv_sEH')\n",
    "    \n",
    "    # save preds（foldごと）\n",
    "    all_preds.append(preds_test) \n",
    "    \n",
    "    df_oof.to_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    \n",
    "    del df_oof\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    df_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    df_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "\n",
    "df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(all_preds, 0)\n",
    "\n",
    "df_test['binds'] = 0\n",
    "df_test.loc[df_test['protein_name']=='BRD4', 'binds'] = preds[df_test['protein_name']=='BRD4', 0]\n",
    "df_test.loc[df_test['protein_name']=='HSA', 'binds'] = preds[df_test['protein_name']=='HSA', 1]\n",
    "df_test.loc[df_test['protein_name']=='sEH', 'binds'] = preds[df_test['protein_name']=='sEH', 2]\n",
    "df_test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / f'submission_fold{fold_id}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 古いckpt pathを削除\n",
    "for fold in range(0, 5): \n",
    "    del_old_ckpt_path(fold, paths.MODEL_WEIGHTS_DIR)\n",
    "    \n",
    "    oof_path = paths.OUTPUT_DIR / f'oof_fold_{fold}.parquet'\n",
    "    oof_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8006601,
     "sourceId": 67356,
     "sourceType": "competition"
    },
    {
     "datasetId": 4914065,
     "sourceId": 8275617,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
