{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leash Bio\n",
    "\n",
    "- 005をChemBEATaに変更\n",
    "- 各building blockごとにembedingして特徴量として使用\n",
    "\n",
    "## ref\n",
    "- https://www.kaggle.com/code/yyyu54/pytorch-version-belka-1dcnn-starter-with-all-data\n",
    "- https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = '007'\n",
    "DEBUG = False\n",
    "data_ratio = 1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# import polars as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "# seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping,\n",
    "    TQDMProgressBar,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    RichModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from funcs.tokenize import tokenize_ChemBEATa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s)\n",
      "pytorch: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def is_kaggle_kernel():\n",
    "    return os.path.exists('/kaggle/working')\n",
    "\n",
    "if is_kaggle_kernel():\n",
    "\n",
    "    BASE_DIR = Path(\"/kaggle\")\n",
    "    DATA_DIR = BASE_DIR / \"input\"\n",
    "    OUTPUT_DIR = BASE_DIR / \"working\"\n",
    "    print('on kaggle notebook')\n",
    "\n",
    "else:\n",
    "    BASE_DIR = Path(os.getcwd()) / './../'\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    OUTPUT_DIR = BASE_DIR / f\"output/exp{exp_no}\"\n",
    "    \n",
    "# set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():    \n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "print('pytorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    SEED = 2024\n",
    "    \n",
    "    PREPROCESS = False\n",
    "    EPOCHS = 20 #20\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 16\n",
    "    \n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "    NUM_FOLDS = 5    \n",
    "    USE_NUM_FOLD = 1\n",
    "    \n",
    "class paths:    \n",
    "    DATA_DIR = DATA_DIR\n",
    "    OUTPUT_DIR = OUTPUT_DIR\n",
    "    MODEL_WEIGHTS_DIR = OUTPUT_DIR / f\"bio-models-exp{exp_no}\"\n",
    "    \n",
    "    SHRUNKEN_DATA_DIR = DATA_DIR / \"shrunken-train-set\"\n",
    "\n",
    "    TRAIN_PATH = SHRUNKEN_DATA_DIR / \"train_fold.parquet\"\n",
    "    TEST_PATH = SHRUNKEN_DATA_DIR / \"test_fold.parquet\"\n",
    "    \n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    }
   ],
   "source": [
    "print('fix seed')\n",
    "\n",
    "def my_seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "# seed_everything(config.SEED, workers=True)\n",
    "my_seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loda Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_cols = ['buildingblock1_smiles', 'buildingblock2_smiles','buildingblock3_smiles', 'fold']\n",
    "TARGETS = ['binds_BRD4', 'binds_HSA','binds_sEH']\n",
    "\n",
    "df_train = pd.read_parquet(paths.TRAIN_PATH, columns=bb_cols + TARGETS)\n",
    "    \n",
    "if DEBUG:\n",
    "    df_train = df_train.sample(100000).reset_index(drop=True)\n",
    "else:\n",
    "    len_train = int(len(df_train)*data_ratio)\n",
    "    df_train = df_train.sample(len_train).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building block smiles\n",
    "# NOTE: trainとtestのindexとsmilesは一致していないっぽい\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_1.p', 'rb') as file:\n",
    "    train_dicts_bb1 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_2.p', 'rb') as file:\n",
    "    train_dicts_bb2 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_3.p', 'rb') as file:\n",
    "    train_dicts_bb3 = pickle.load(file)\n",
    "\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_1_test.p', 'rb') as file:\n",
    "    test_dicts_bb1 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_2_test.p', 'rb') as file:\n",
    "    test_dicts_bb2 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_3_test.p', 'rb') as file:\n",
    "    test_dicts_bb3= pickle.load(file)\n",
    "    \n",
    "# bb1のidxをscaffoldのidxに変換するdict\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_idx_to_scaffold_idx_dict_1.p', mode='rb') as file:\n",
    "    test_bb1idx2scaidx= pickle.load(file)\n",
    "\n",
    "test_dicts_bb1_reverse = {val:key for key, val in test_dicts_bb1.items()}\n",
    "test_dicts_bb2_reverse = {val:key for key, val in test_dicts_bb2.items()}\n",
    "test_dicts_bb3_reverse = {val:key for key, val in test_dicts_bb3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(paths.DATA_DIR / 'test.parquet')\n",
    "df_test.drop(['molecule_smiles'], axis=1, inplace=True)\n",
    "\n",
    "df_test['buildingblock1_smiles'] = df_test['buildingblock1_smiles'].map(test_dicts_bb1_reverse)\n",
    "df_test['buildingblock2_smiles'] = df_test['buildingblock2_smiles'].map(test_dicts_bb2_reverse)\n",
    "df_test['buildingblock3_smiles'] = df_test['buildingblock3_smiles'].map(test_dicts_bb3_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenize smiles\n",
    "df_train_bb1 = tokenize_ChemBEATa(train_dicts_bb1)\n",
    "df_train_bb2 = tokenize_ChemBEATa(train_dicts_bb2)\n",
    "df_train_bb3 = tokenize_ChemBEATa(train_dicts_bb3)\n",
    "df_test_bb1 = tokenize_ChemBEATa(test_dicts_bb1)\n",
    "df_test_bb2 = tokenize_ChemBEATa(test_dicts_bb2)\n",
    "df_test_bb3 = tokenize_ChemBEATa(test_dicts_bb3)\n",
    "\n",
    "input_len = df_train_bb1.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset & DataModule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        df_bb1: pd.DataFrame,\n",
    "        df_bb2: pd.DataFrame,\n",
    "        df_bb3: pd.DataFrame,\n",
    "        mode = 'train'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        meta_cols = [\"buildingblock1_smiles\", \"buildingblock2_smiles\", \"buildingblock3_smiles\"]\n",
    "        if (self.mode == 'train') or (self.mode == 'valid'):\n",
    "            meta_cols += TARGETS\n",
    "            \n",
    "        self.df = df[meta_cols].values\n",
    "        self.bb1_array = df_bb1.values\n",
    "        self.bb2_array = df_bb2.values\n",
    "        self.bb3_array = df_bb3.values\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.df[index, :]\n",
    "\n",
    "        x1 = self.bb1_array[row[0], :]\n",
    "        x2 = self.bb2_array[row[1], :]\n",
    "        x3 = self.bb3_array[row[2], :]\n",
    "        X = np.concatenate([x1, x2, x3])\n",
    "        \n",
    "        if (self.mode == 'train') or (self.mode == 'valid'):\n",
    "            y = row[-3:]\n",
    "        else:\n",
    "            y = np.zeros(3)\n",
    "        \n",
    "        output = {\n",
    "            'X': torch.tensor(X, dtype=torch.float32),\n",
    "            'y': torch.tensor(y, dtype=torch.float16)\n",
    "        }        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dataset\n",
    "if DEBUG:\n",
    "    dataset = BioDataset(df_train, df_train_bb1, df_train_bb2, df_train_bb3, mode='valid')\n",
    "    X = dataset[0]['X']\n",
    "    y = dataset[0]['y']\n",
    "    print(X.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning data module\n",
    "class BioDataModule(LightningDataModule):\n",
    "    def __init__(self, df_train, fold_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = df_train[df_train['fold'] != fold_id]\n",
    "        self.valid_df = df_train[df_train['fold'] == fold_id]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = BioDataset(self.train_df, df_train_bb1, df_train_bb2, df_train_bb3, mode='train')\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=config.BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=config.NUM_WORKERS,\n",
    "                                pin_memory=True,\n",
    "                                persistent_workers=True,\n",
    "                                drop_last=True,\n",
    "                            )\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = BioDataset(self.valid_df, df_train_bb1, df_train_bb2, df_train_bb3, mode='valid')\n",
    "        valid_dataloader = torch.utils.data.DataLoader(\n",
    "                                            valid_dataset,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config.NUM_WORKERS,\n",
    "                                            pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            drop_last=False,\n",
    "                                        )\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_len_per_bb=input_len,\n",
    "                 num_filters=32,\n",
    "                 output_dim=3):\n",
    "        super(BioModel, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.input_len = input_len_per_bb\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # embedding層は共通で持つ\n",
    "        # self.embedding = nn.Embedding(num_embeddings=self.input_dim_embedding, embedding_dim=self.hidden_dim, padding_idx=0)\n",
    "        \n",
    "        # feature extractor 層は各building block1と2, 3で分ける\n",
    "        self.feature_extractor_bb1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=self.num_filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.num_filters, out_channels=self.num_filters*2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.num_filters*2, out_channels=self.num_filters*3, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        self.feature_extractor_bb23 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=self.num_filters, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.num_filters, out_channels=self.num_filters*2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.num_filters*2, out_channels=self.num_filters*3, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.num_filters * 3 * 3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.output = nn.Linear(512, self.output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # feature extractor\n",
    "        x1 = x[:, :self.input_len].unsqueeze(1)\n",
    "        x2 = x[:, self.input_len:self.input_len*2].unsqueeze(1)\n",
    "        x3 = x[:, self.input_len*2:].unsqueeze(1)\n",
    "\n",
    "        x1 = self.feature_extractor_bb1(x1).squeeze(-1)\n",
    "        x2 = self.feature_extractor_bb23(x2).squeeze(-1)\n",
    "        x3 = self.feature_extractor_bb23(x3).squeeze(-1)\n",
    "                \n",
    "        # concat\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model\n",
    "if DEBUG:\n",
    "    dummy_model = BioModel()\n",
    "    total_params = sum(p.numel() for p in dummy_model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    \n",
    "    dummy_input = torch.rand((64, 1152), dtype=torch.float32)\n",
    "    output = dummy_model(dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(y_preds, y_true):\n",
    "    score_BRD4 = APS(y_true[:,0], y_preds[:,0])\n",
    "    score_HSA = APS(y_true[:,1], y_preds[:,1])\n",
    "    score_sEH = APS(y_true[:,2], y_preds[:,2])\n",
    "    score = (score_BRD4 + score_HSA + score_sEH) / 3\n",
    "    \n",
    "    return score_BRD4, score_HSA, score_sEH, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BioModule, self).__init__()\n",
    "       \n",
    "        self.model = BioModel()\n",
    "        self.validation_step_outputs = []\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pred = self.model(X)\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'valid_loss_epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        train_loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log('train_loss', train_loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "        \n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        valid_loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log('valid_loss', valid_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "        \n",
    "        self.validation_step_outputs.append({\"valid_loss\":valid_loss, \"preds\":preds, \"targets\":y})\n",
    "        \n",
    "        return valid_loss\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def calc_score(self, y_preds, y_true):\n",
    "        return calc_score(y_preds, y_true)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        # 各iterationごとのlossを平均\n",
    "        avg_loss = torch.stack([x['valid_loss'] for x in outputs]).mean()\n",
    "        self.log(\"valid_loss_epoch\", avg_loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        # scoreを計算\n",
    "        y_preds = torch.cat([x['preds'] for x in outputs],dim=0).detach().cpu().numpy()\n",
    "        y_true = torch.cat([x['targets'] for x in outputs],dim=0).detach().cpu().numpy()\n",
    "\n",
    "        score = self.calc_score(y_preds, y_true)[-1]\n",
    "        self.log(\"valid_score\", score, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "        \n",
    "        return {'valid_loss_epoch': avg_loss, \"valid_score\":score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_ckpt_path(fold):\n",
    "    ckpt_list = list(paths.MODEL_WEIGHTS_DIR.glob(f'fold_{fold}*.ckpt'))\n",
    "\n",
    "    latest_ckpt = max(ckpt_list, key=lambda p: p.stat().st_mtime)\n",
    "    print(f'Latest checkpoint file: {latest_ckpt}')\n",
    "    return latest_ckpt\n",
    "\n",
    "\n",
    "def old_ckpt_path(fold):\n",
    "    # Assuming paths.CKPT_ROOT and fold_num are already defined\n",
    "    ckpt_list = list(paths.MODEL_WEIGHTS_DIR.glob(f'fold_{fold}*.ckpt'))\n",
    "    print(f'find {len(ckpt_list)} ckpts')\n",
    "    print(ckpt_list)\n",
    "\n",
    "    # Find the latest checkpoint file\n",
    "    if ckpt_list:\n",
    "        latest_ckpt = max(ckpt_list, key=lambda p: p.stat().st_mtime)\n",
    "        print(f'Latest checkpoint file: {latest_ckpt}')\n",
    "\n",
    "        # Delete all other checkpoint files\n",
    "        for ckpt in ckpt_list:\n",
    "            if ckpt != latest_ckpt:\n",
    "                print(f'Deleting checkpoint file: {ckpt}')\n",
    "                ckpt.unlink()\n",
    "    else:\n",
    "        print('No checkpoint files found')\n",
    "    \n",
    "\n",
    "\n",
    "def predict_in_batches(model, df, df_bb1, df_bb2, df_bb3, mode):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = BioDataset(df, df_bb1, df_bb2, df_bb3, mode=mode)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                                        dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=config.NUM_WORKERS,\n",
    "                                        pin_memory=True,\n",
    "                                        persistent_workers=True,\n",
    "                                        drop_last=False,\n",
    "                                    )\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['X'].to(device)\n",
    "            logits = model(inputs)\n",
    "            preds = torch.sigmoid(logits)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, df, infer=False):\n",
    "    print(f\"======== Running training for fold {fold_id} =============\")\n",
    "    \n",
    "    # == init data module and model ==\n",
    "    model = BioModule()\n",
    "    datamodule = BioDataModule(df, fold_id)\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                                        monitor='valid_score',\n",
    "#                                             monitor='valid_loss_epoch',\n",
    "                                          dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='max'\n",
    "                                          )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='valid_score',\n",
    "#         monitor=\"valid_loss_epoch\", \n",
    "        mode=\"max\", \n",
    "        patience=5,\n",
    "        verbose=True\n",
    "        )\n",
    "    callbacks_to_use = [checkpoint_callback,\n",
    "                        early_stop_callback,\n",
    "                        RichModelSummary(),\n",
    "                        RichProgressBar(),\n",
    "                       ]\n",
    "\n",
    "    # == init trainer ==\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        callbacks=callbacks_to_use,\n",
    "        accelerator=device,\n",
    "        devices=-1,  # 全ての利用可能なGPUを使用\n",
    "        deterministic=False,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}'),\n",
    "    )\n",
    "    \n",
    "    if not infer:\n",
    "        # == Training ==\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        weights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "    else:\n",
    "        ckpt_path = find_latest_ckpt_path(fold_id) \n",
    "        weights = torch.load(ckpt_path)['state_dict']\n",
    "        \n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    valid_df = datamodule.valid_df\n",
    "    \n",
    "    preds_oof = predict_in_batches(model, valid_df, df_train_bb1, df_train_bb2, df_train_bb3, mode='valid')\n",
    "    y_oof = valid_df[TARGETS].values\n",
    "    \n",
    "    score_BRD4, score_HSA, score_sEH, score = calc_score(preds_oof, y_oof)\n",
    "    \n",
    "    valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
    "    \n",
    "    print(f'fold:{fold_id} | CV score = {score}')\n",
    "    \n",
    "    df_test_temp = df_test.drop(['id'], axis=1)\n",
    "    preds_test = predict_in_batches(model, df_test_temp, df_test_bb1, df_test_bb2, df_test_bb3, mode='test')\n",
    "    \n",
    "    del model, datamodule, trainer, preds_oof, y_oof\n",
    "    gc.collect()\n",
    "    \n",
    "    score_dict = {\n",
    "        'BRD4':score_BRD4,\n",
    "        \"HSA\":score_HSA,\n",
    "        \"sEH\":score_sEH,\n",
    "        \"all\":score\n",
    "    }\n",
    "    \n",
    "    return preds_test, score_dict, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running training for fold 0 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp007/bio-models-exp007/fold_0-v13.ckpt\n",
      "fold:0 | CV score = 0.21401210723799427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running training for fold 1 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp007/bio-models-exp007/fold_1-v3.ckpt\n",
      "fold:1 | CV score = 0.2609070797843028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running training for fold 2 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp007/bio-models-exp007/fold_2-v3.ckpt\n",
      "fold:2 | CV score = 0.22950094232424553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running training for fold 3 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp007/bio-models-exp007/fold_3-v3.ckpt\n",
      "fold:3 | CV score = 0.2377467678992126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running training for fold 4 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp007/bio-models-exp007/fold_4-v1.ckpt\n",
      "fold:4 | CV score = 0.2848297637641219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
      "/tmp/ipykernel_2463081/2156084032.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_oof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mNUM_FOLDS):\n\u001b[1;32m     46\u001b[0m     df_temp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(paths\u001b[38;5;241m.\u001b[39mOUTPUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moof_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     df_oof_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdf_oof\u001b[49m, df_temp], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m     df_oof_all\u001b[38;5;241m.\u001b[39mto_parquet(paths\u001b[38;5;241m.\u001b[39mOUTPUT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moof_all.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_oof' is not defined"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# tokenizerの warning対策\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "all_preds = []\n",
    "score_list = []\n",
    "score_list_BRD4 = []\n",
    "score_list_HSA = []\n",
    "score_list_sEH = []\n",
    "\n",
    "def save_list_by_text(score_list, filename):\n",
    "    # ファイルに書き込み\n",
    "    score_list_txt = [str(loss) for loss in score_list]\n",
    "    with open(paths.OUTPUT_DIR / f'{filename}.txt', 'w') as file:\n",
    "        file.write(', '.join(score_list_txt))\n",
    "    \n",
    "\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    \n",
    "    preds_test, score_dict, df_oof = run_training(fold_id, df_train, infer=True)\n",
    "    \n",
    "    # save score\n",
    "    score_list_BRD4.append(score_dict['BRD4'])\n",
    "    score_list_HSA.append(score_dict['HSA'])\n",
    "    score_list_sEH.append(score_dict['sEH'])\n",
    "    score_list.append(score_dict['all'])\n",
    "    \n",
    "    save_list_by_text(score_list, 'cv_all')\n",
    "    save_list_by_text(score_list_BRD4, 'cv_BRD4')\n",
    "    save_list_by_text(score_list_HSA, 'cv_HSA')\n",
    "    save_list_by_text(score_list_sEH, 'cv_sEH')\n",
    "    \n",
    "    # save preds（foldごと）\n",
    "    all_preds.append(preds_test) \n",
    "    \n",
    "    df_oof.to_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    \n",
    "    del df_oof\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    df_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    df_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "    \n",
    "df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    df_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    df_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "    df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(all_preds, 0)\n",
    "\n",
    "df_test['binds'] = 0\n",
    "df_test.loc[df_test['protein_name']=='BRD4', 'binds'] = preds[df_test['protein_name']=='BRD4', 0]\n",
    "df_test.loc[df_test['protein_name']=='HSA', 'binds'] = preds[df_test['protein_name']=='HSA', 1]\n",
    "df_test.loc[df_test['protein_name']=='sEH', 'binds'] = preds[df_test['protein_name']=='sEH', 2]\n",
    "df_test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / f'submission_fold{fold_id}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for fold in range(0, 5):    \n",
    "    # 古いckpt pathを削除\n",
    "    old_ckpt_path(fold)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8006601,
     "sourceId": 67356,
     "sourceType": "competition"
    },
    {
     "datasetId": 4914065,
     "sourceId": 8275617,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
