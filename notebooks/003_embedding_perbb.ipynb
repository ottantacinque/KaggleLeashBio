{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leash Bio\n",
    "\n",
    "- [DataSet](https://www.kaggle.com/datasets/ahmedelfazouan/belka-enc-dataset)\n",
    "- 各building blockごとにembedingして特徴量として使用\n",
    "- simple 1dcnn model trained on 30 epochs.\n",
    "\n",
    "## ref\n",
    "- https://www.kaggle.com/code/yyyu54/pytorch-version-belka-1dcnn-starter-with-all-data\n",
    "- https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook\n",
    "\n",
    "- Notes: the embedding layer in pytorch is different than tensorflow, in which it doesn't have the mask_zero option, so I had to change the num of embedding to 37 to make it work. Please let me know if there's a better way to implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = '003'\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_lightning\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# import polars as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "# seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping,\n",
    "    TQDMProgressBar,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    RichModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from funcs.tokenize import tokenize_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPU(s)\n",
      "pytorch: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def is_kaggle_kernel():\n",
    "    return os.path.exists('/kaggle/working')\n",
    "\n",
    "if is_kaggle_kernel():\n",
    "\n",
    "    BASE_DIR = Path(\"/kaggle\")\n",
    "    DATA_DIR = BASE_DIR / \"input\"\n",
    "    OUTPUT_DIR = BASE_DIR / \"working\"\n",
    "    print('on kaggle notebook')\n",
    "\n",
    "else:\n",
    "    BASE_DIR = Path(os.getcwd()) / './../'\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    OUTPUT_DIR = BASE_DIR / f\"output/exp{exp_no}\"\n",
    "    \n",
    "# set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():    \n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "print('pytorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    SEED = 2024\n",
    "    \n",
    "    PREPROCESS = False\n",
    "    EPOCHS = 30 #20\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 8\n",
    "    \n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "    NUM_FOLDS = 5    \n",
    "    USE_NUM_FOLD = 1\n",
    "    \n",
    "class paths:    \n",
    "    DATA_DIR = DATA_DIR\n",
    "    TRAIN_PATH = DATA_DIR / \"train.parquet\"\n",
    "    TEST_PATH = DATA_DIR / \"test.parquet\"\n",
    "    OUTPUT_DIR = OUTPUT_DIR\n",
    "    MODEL_WEIGHTS_DIR = OUTPUT_DIR / f\"bio-models-exp{exp_no}\"\n",
    "    \n",
    "    SHRUNKEN_DATA_DIR = DATA_DIR / \"shrunken-train-set\"\n",
    "    \n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    }
   ],
   "source": [
    "print('fix seed')\n",
    "\n",
    "def my_seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "# seed_everything(config.SEED, workers=True)\n",
    "my_seed_everything(config.SEED)\n",
    "\n",
    "# FEATURES = [f'enc{i}' for i in range(142)]\n",
    "TARGETS = ['bind1', 'bind2', 'bind3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loda Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_cols = ['buildingblock1_smiles', 'buildingblock2_smiles','buildingblock3_smiles']\n",
    "TARGETS = ['binds_BRD4', 'binds_HSA','binds_sEH']\n",
    "\n",
    "df_train = pd.read_parquet(paths.DATA_DIR / 'shrunken-train-set/train.parquet', columns=bb_cols + TARGETS)\n",
    "df_test = pd.read_parquet(paths.DATA_DIR / 'shrunken-train-set/test.parquet', columns=bb_cols)\n",
    "    \n",
    "if DEBUG:\n",
    "    df_train = df_train.sample(100000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building block smiles\n",
    "# NOTE: trainとtestのindexとsmilesは一致していないっぽい\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_1.p', 'rb') as file:\n",
    "    train_dicts_bb1 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_2.p', 'rb') as file:\n",
    "    train_dicts_bb2 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'train_dicts/BBs_dict_reverse_3.p', 'rb') as file:\n",
    "    train_dicts_bb3 = pickle.load(file)\n",
    "\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_1_test.p', 'rb') as file:\n",
    "    test_dicts_bb1 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_2_test.p', 'rb') as file:\n",
    "    test_dicts_bb2 = pickle.load(file)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'test_dicts/BBs_dict_reverse_3_test.p', 'rb') as file:\n",
    "    test_dicts_bb3= pickle.load(file)\n",
    "    \n",
    "trian_dict_list = [train_dicts_bb1, train_dicts_bb2, train_dicts_bb3]\n",
    "test_dict_list = [test_dicts_bb1, test_dicts_bb2, test_dicts_bb3]\n",
    "all_dict_list = trian_dict_list + test_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fold\n",
    "skf = StratifiedKFold(n_splits=config.NUM_FOLDS, shuffle=True, random_state=42)\n",
    "folds_list = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train[TARGETS].sum(1))):\n",
    "    folds_list.append((train_idx, valid_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize smiles\n",
    "df_train_bb1 = tokenize_smiles(train_dicts_bb1)\n",
    "df_train_bb2 = tokenize_smiles(train_dicts_bb2)\n",
    "df_train_bb3 = tokenize_smiles(train_dicts_bb3)\n",
    "df_test_bb1 = tokenize_smiles(test_dicts_bb1)\n",
    "df_test_bb2 = tokenize_smiles(test_dicts_bb2)\n",
    "df_test_bb3 = tokenize_smiles(test_dicts_bb3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset & DataModule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        df_bb1: pd.DataFrame,\n",
    "        df_bb2: pd.DataFrame,\n",
    "        df_bb3: pd.DataFrame,\n",
    "        mode = 'train'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.bb1_array = df_bb1.values\n",
    "        self.bb2_array = df_bb2.values\n",
    "        self.bb3_array = df_bb3.values\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.df.loc[index, :]\n",
    "        \n",
    "        bb1_idx = row[\"buildingblock1_smiles\"]\n",
    "        bb2_idx = row[\"buildingblock2_smiles\"]\n",
    "        bb3_idx = row[\"buildingblock3_smiles\"]\n",
    "\n",
    "        x1 = self.bb1_array[bb1_idx, :]\n",
    "        x2 = self.bb2_array[bb2_idx, :]\n",
    "        x3 = self.bb3_array[bb3_idx, :]\n",
    "        X = np.concatenate([x1, x2, x3])\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            y = row[TARGETS]\n",
    "        else:\n",
    "            y = np.zeros(3)\n",
    "        \n",
    "        output = {\n",
    "            'X': torch.tensor(X, dtype=torch.int),\n",
    "            'y': torch.tensor(y, dtype=torch.float16)\n",
    "        }        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Check Dataset\n",
    "if DEBUG:\n",
    "    dataset = BioDataset(df_train, df_train_bb1, df_train_bb2, df_train_bb3, mode='valid')\n",
    "    X = dataset[0]['X']\n",
    "    y = dataset[0]['y']\n",
    "    print(X.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning data module\n",
    "class BioDataModule(LightningDataModule):\n",
    "    def __init__(self, df_train, train_idx, valid_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = df_train.iloc[train_idx, :]\n",
    "        self.valid_df = df_train.iloc[valid_idx, :]\n",
    "        self.test_df = df_test.copy()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = BioDataset(self.train_df, df_train_bb1, df_train_bb2, df_train_bb3, mode='train')\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=config.BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=config.NUM_WORKERS,\n",
    "                                pin_memory=True,\n",
    "                                persistent_workers=True,\n",
    "                                drop_last=True,\n",
    "                            )\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = BioDataset(self.valid_df, df_train_bb1, df_train_bb2, df_train_bb3, mode='valid')\n",
    "        valid_dataloader = torch.utils.data.DataLoader(\n",
    "                                            valid_dataset,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config.NUM_WORKERS,\n",
    "                                            pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            drop_last=False,\n",
    "                                        )\n",
    "        return valid_dataloader\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     test_dataset = BioDataset(self.test_df, df_test_bb1, df_test_bb2, df_test_bb3, mode='valid')\n",
    "    #     test_dataloader = torch.utils.data.DataLoader(\n",
    "    #                                         test_dataset,\n",
    "    #                                         batch_size=config.BATCH_SIZE,\n",
    "    #                                         shuffle=False,\n",
    "    #                                         num_workers=config.NUM_WORKERS,\n",
    "    #                                         pin_memory=True,\n",
    "    #                                         persistent_workers=True,\n",
    "    #                                         drop_last=False\n",
    "    #                                     )\n",
    "    #     return test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim=240, \n",
    "                 input_dim_embedding=37, \n",
    "                 hidden_dim=128, \n",
    "                 num_filters=32, \n",
    "                 output_dim=3):\n",
    "        super(BioModel, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.input_dim_embedding = input_dim_embedding\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.input_dim_embedding, embedding_dim=self.hidden_dim, padding_idx=0)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=self.hidden_dim, out_channels=self.num_filters, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.num_filters, out_channels=self.num_filters*2, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.num_filters*2, out_channels=self.num_filters*3, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.num_filters*3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output = nn.Linear(512, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = self.global_max_pool(x).squeeze(2)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = self.output(x)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1717059\n",
      "torch.Size([1, 3])\n",
      "tensor([[-0.0163,  0.0252, -0.0121]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# check model\n",
    "if DEBUG:\n",
    "    dummy_model = BioModel()\n",
    "    total_params = sum(p.numel() for p in dummy_model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "    dummy_input = torch.randint(0, 37, (1, 240), dtype=torch.long)\n",
    "    output = dummy_model(dummy_input)\n",
    "    print(output.shape)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BioModule, self).__init__()\n",
    "       \n",
    "        self.model = BioModel()\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pred = self.model(X)\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss_epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        train_loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        \n",
    "        self.log('train_loss', train_loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "        \n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        valid_loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        \n",
    "        self.log('valid_loss', valid_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "        \n",
    "        self.validation_step_outputs.append({\"valid_loss\":valid_loss})\n",
    "        \n",
    "        return valid_loss\n",
    "    \n",
    "    # def test_step(self, batch, batch_idx):\n",
    "\n",
    "    #     X, y = batch.pop('X'), batch.pop('y')\n",
    "    #     logits = self(X)\n",
    "    #     pred = torch.sigmoid(logits)\n",
    "        \n",
    "    #     return pred\n",
    "       \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return self._test_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        # 各iterationごとのlossを平均\n",
    "        avg_loss = torch.stack([x['valid_loss'] for x in outputs]).mean()\n",
    "        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True, logger=True)   \n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "        \n",
    "        return {'val_loss': avg_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, df, df_bb1, df_bb2, df_bb3):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = BioDataset(df, df_bb1, df_bb2, df_bb3, mode='valid')\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                                        dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=config.NUM_WORKERS,\n",
    "                                        pin_memory=True,\n",
    "                                        persistent_workers=True,\n",
    "                                        drop_last=False,\n",
    "                                    )\n",
    "    \n",
    "    # test_dataset = TensorDataset(torch.tensor(df[FEATURES].values, dtype=torch.int))\n",
    "    # test_loader = DataLoader(test_dataset,\n",
    "    #                          batch_size=config.BATCH_SIZE, \n",
    "    #                          shuffle=False, \n",
    "    #                          num_workers=config.NUM_WORKERS, \n",
    "    #                          pin_memory=True)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0].to(device)\n",
    "            preds = model(inputs)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, folds_list, df):\n",
    "    print('================================================================')\n",
    "    print(f\"==== Running training for fold {fold_id} ====\")\n",
    "    \n",
    "    # == init data module and model ==\n",
    "    train_idx, valid_idx = folds_list[fold_id]\n",
    "    model = BioModule()\n",
    "    datamodule = BioDataModule(df, train_idx, valid_idx)\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss_epoch',\n",
    "                                          dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='min')\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss_epoch\", mode=\"min\", patience=5, verbose=True)\n",
    "    callbacks_to_use = [checkpoint_callback,\n",
    "                        early_stop_callback,\n",
    "                        RichModelSummary(),\n",
    "                        RichProgressBar(),\n",
    "#                         TQDMProgressBar(refresh_rate=1)\n",
    "                       ]\n",
    "\n",
    "    # == init trainer ==\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        callbacks=callbacks_to_use,\n",
    "        accelerator=device,\n",
    "        deterministic=False,\n",
    "        gradient_clip_val=10,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}'),\n",
    "    )\n",
    "    \n",
    "    # == Training ==\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    \n",
    "    # == Prediction by best model==\n",
    "    weights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    valid_df = datamodule.valid_df\n",
    "    \n",
    "    preds_oof = predict_in_batches(model, valid_df, df_train_bb1, df_train_bb2, df_train_bb3)\n",
    "    y_oof = valid_df[TARGETS].values\n",
    "    score = APS(y_oof, preds_oof, average='micro')\n",
    "    \n",
    "    print(f'fold:{fold} | CV score = {score}')\n",
    "    \n",
    "    preds_test = predict_in_batches(model, df_test, df_test_bb1, df_test_bb2, df_test_bb3)\n",
    "    \n",
    "    del model, datamodule, trainer, preds_oof, y_oof\n",
    "    gc.collect()\n",
    "    \n",
    "    return score, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "==== Running training for fold 0 ====\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "`RichModelSummary` requires `rich` to be installed. Install it by running `pip install -U rich`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m score_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mNUM_FOLDS):\n\u001b[0;32m----> 7\u001b[0m     score, preds_test \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     score_list\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m     10\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mappend(preds_test)\n",
      "Cell \u001b[0;32mIn[117], line 21\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(fold_id, folds_list, df)\u001b[0m\n\u001b[1;32m     11\u001b[0m     checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m                                           dirpath\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mMODEL_WEIGHTS_DIR,\n\u001b[1;32m     13\u001b[0m                                           save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                           filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                                           mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m     early_stop_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m     callbacks_to_use \u001b[38;5;241m=\u001b[39m [checkpoint_callback,\n\u001b[1;32m     20\u001b[0m                         early_stop_callback,\n\u001b[0;32m---> 21\u001b[0m                         \u001b[43mRichModelSummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     22\u001b[0m                         RichProgressBar(),\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#                         TQDMProgressBar(refresh_rate=1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m                        ]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# == init trainer ==\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     28\u001b[0m         max_epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mEPOCHS,\n\u001b[1;32m     29\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks_to_use,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m         logger\u001b[38;5;241m=\u001b[39mTensorBoardLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     35\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_lightning/callbacks/rich_model_summary.py:62\u001b[0m, in \u001b[0;36mRichModelSummary.__init__\u001b[0;34m(self, max_depth, **summarize_kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_depth: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msummarize_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _RICH_AVAILABLE:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`RichModelSummary` requires `rich` to be installed. Install it by running `pip install -U rich`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(max_depth, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msummarize_kwargs)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: `RichModelSummary` requires `rich` to be installed. Install it by running `pip install -U rich`."
     ]
    }
   ],
   "source": [
    "# training\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "all_preds = []\n",
    "score_list = []\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    score, preds_test = run_training(fold_id, folds_list, df_train)\n",
    "    \n",
    "    score_list.append(score)\n",
    "    all_preds.append(preds_test)\n",
    "    \n",
    "    # ファイルに書き込み\n",
    "    score_list = [str(loss) for loss in score_list]\n",
    "    with open(paths.OUTPUT_DIR / 'cv_result.txt', 'w') as file:\n",
    "        file.write(', '.join(score_list))\n",
    "    \n",
    "    break\n",
    "        \n",
    "preds = np.mean(all_preds, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(paths.DATA_DIR / 'test.parquet')\n",
    "test['binds'] = 0\n",
    "test.loc[test['protein_name']=='BRD4', 'binds'] = preds[(test['protein_name']=='BRD4').values, 0]\n",
    "test.loc[test['protein_name']=='HSA', 'binds'] = preds[(test['protein_name']=='HSA').values, 1]\n",
    "test.loc[test['protein_name']=='sEH', 'binds'] = preds[(test['protein_name']=='sEH').values, 2]\n",
    "test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / 'submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8006601,
     "sourceId": 67356,
     "sourceType": "competition"
    },
    {
     "datasetId": 4914065,
     "sourceId": 8275617,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
