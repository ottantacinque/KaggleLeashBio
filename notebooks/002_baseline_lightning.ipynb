{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Leash Bio\n","\n","- [DataSet](https://www.kaggle.com/datasets/ahmedelfazouan/belka-enc-dataset)\n","- 分子全体のsmilesをembeddingしたものを特徴料で使用\n","- simple 1dcnn model trained on 20 epochs.\n","\n","## ref\n","- https://www.kaggle.com/code/yyyu54/pytorch-version-belka-1dcnn-starter-with-all-data\n","- https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook\n","\n","- Notes: the embedding layer in pytorch is different than tensorflow, in which it doesn't have the mask_zero option, so I had to change the num of embedding to 37 to make it work. Please let me know if there's a better way to implement it!"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["exp_no = '002'\n","DEBUG = True"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# !pip install fastparquet -q\n","# ! pip install --quiet \"ipython[notebook]>=8.0.0, <8.12.0\" \"lightning>=2.0.0rc0\" \"setuptools==67.4.0\" \"torch>=1.8.1, <1.14.0\" \"torchvision\" \"pytorch-lightning>=1.4, <2.0.0\" \"torchmetrics>=0.7, <0.12\"\n","# ! pip install -U torch_xla -q"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import gc\n","import os\n","import pickle\n","import random\n","import joblib\n","import pandas as pd\n","# import polars as pd\n","from tqdm import tqdm\n","\n","import numpy as np\n","from sklearn.metrics import average_precision_score as APS\n","from sklearn.model_selection import StratifiedKFold\n","\n","import torch\n","from torch.utils.data import TensorDataset, Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n","\n","from pytorch_lightning import LightningModule\n","from pytorch_lightning import LightningDataModule, Trainer, seed_everything\n","from pytorch_lightning.callbacks import (\n","    ModelCheckpoint, \n","    EarlyStopping,\n","    TQDMProgressBar,\n","    LearningRateMonitor,\n","    ModelCheckpoint,\n","    RichModelSummary,\n","    RichProgressBar,\n",")\n","from pytorch_lightning.loggers import TensorBoardLogger"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 0 GPU(s)\n","pytorch: 2.3.0\n"]}],"source":["import os\n","from pathlib import Path\n","\n","def is_kaggle_kernel():\n","    return os.path.exists('/kaggle/working')\n","\n","if is_kaggle_kernel():\n","\n","    BASE_DIR = Path(\"/kaggle\")\n","    DATA_DIR = BASE_DIR / \"input\"\n","    OUTPUT_DIR = BASE_DIR / \"working\"\n","    print('on kaggle notebook')\n","\n","else:\n","    BASE_DIR = Path(os.getcwd()) / './../'\n","    DATA_DIR = BASE_DIR / \"data\"\n","    OUTPUT_DIR = BASE_DIR / \"output/exp{exp_no}\"\n","    \n","# set device\n","if torch.backends.mps.is_available():\n","    device = \"mps\"\n","elif torch.cuda.is_available():    \n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","    \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","print('Using', torch.cuda.device_count(), 'GPU(s)')\n","print('pytorch:', torch.__version__)"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[],"source":["class config:\n","    SEED = 2024\n","    \n","    PREPROCESS = False\n","    EPOCHS = 30 #20\n","    BATCH_SIZE = 4096\n","    NUM_WORKERS = 16\n","    \n","    LR = 1e-3\n","    WEIGHT_DECAY = 1e-6\n","    MIXED_PRECISION = True\n","    \n","    NUM_FOLDS = 5    \n","    USE_NUM_FOLD = 1\n","    \n","class paths:    \n","    DATA_DIR = DATA_DIR\n","    TRAIN_PATH = DATA_DIR / \"train.parquet\"\n","    TEST_PATH = DATA_DIR / \"test.parquet\"\n","    OUTPUT_DIR = OUTPUT_DIR\n","    MODEL_WEIGHTS_DIR = OUTPUT_DIR / \"bio-models-exp{exp_no}\"\n","    \n","    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"]},{"cell_type":"code","execution_count":40,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Seed set to 2024\n"]},{"name":"stdout","output_type":"stream","text":["fix seed\n"]}],"source":["print('fix seed')\n","seed_everything(config.SEED, workers=True)\n","\n","FEATURES = [f'enc{i}' for i in range(142)]\n","TARGETS = ['bind1', 'bind2', 'bind3']"]},{"cell_type":"markdown","metadata":{},"source":["# **Loda Data**"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["df_train = pd.read_parquet(paths.DATA_DIR / 'belka-enc-dataset/train_enc.parquet')\n","df_test = pd.read_parquet(paths.DATA_DIR / 'belka-enc-dataset/test_enc.parquet')\n","    \n","df_train = df_train.sample(100000).reset_index(drop=True)\n","df_test = df_test.sample(100000).reset_index(drop=True)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# set fold\n","skf = StratifiedKFold(n_splits=config.NUM_FOLDS, shuffle=True, random_state=42)\n","folds_list = []\n","for fold, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train[TARGETS].sum(1))):\n","    folds_list.append((train_idx, valid_idx))\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Dataset & DataModule**"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class BioDataset(torch.utils.data.Dataset):\n","    \n","    def __init__(\n","        self,\n","        array: np.ndarray,\n","        mode: str,\n","    ):\n","        super().__init__()\n","        self.array = array\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.array)\n","    \n","    def __getitem__(self, index):\n","        \n","        if self.mode=='train':\n","            X = self.array[index, :-3]\n","            y = self.array[index, -3:]\n","        else:\n","            X = self.array[index, :]\n","            y = np.zeros(3)\n","        \n","        output = {\n","            'X': torch.tensor(X, dtype=torch.float32),\n","            'y': torch.tensor(y, dtype=torch.float32),\n","        }        \n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["{'X': tensor([ 8., 12., 27., 12., 12., 17., 33., 12., 18., 35., 12., 17., 33., 12.,\n","          4., 12., 12., 12., 35., 12.,  4.,  8., 19., 35., 12., 17., 33., 12.,\n","          4., 12., 17.,  7., 19., 12., 12., 12., 17., 31.,  9., 19., 12.,  4.,\n","          8., 17., 26., 28., 19., 33., 29., 30.,  2., 32., 19., 35., 18., 19.,\n","         35., 29., 35.,  5., 32., 27.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","          0.,  0.]),\n"," 'y': tensor([0., 0., 0.])}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Check Dataset\n","dataset = BioDataset(df_train.values, 'train')\n","dataset[0]"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# lightning data module\n","class BioDataModule(LightningDataModule):\n","    def __init__(self, df, train_idx, valid_idx):\n","        super().__init__()\n","        \n","        self.train_df = df.iloc[train_idx, :]\n","        self.valid_df = df.iloc[valid_idx, :]\n","\n","    def train_dataloader(self):\n","        train_dataset = BioDataset(self.train_df, 'train')\n","        train_dataloader = torch.utils.data.DataLoader(\n","                                train_dataset,\n","                                batch_size=config.BATCH_SIZE,\n","                                shuffle=True,\n","                                num_workers=config.NUM_WORKERS,\n","                                pin_memory=True,\n","                                persistent_workers=True\n","                            )\n","        return train_dataloader\n","\n","    def val_dataloader(self):\n","        valid_dataset = BioDataset(self.valid_df, 'valid')\n","        valid_dataloader = torch.utils.data.DataLoader(\n","                                            valid_dataset,\n","                                            batch_size=config.BATCH_SIZE,\n","                                            shuffle=False,\n","                                            num_workers=config.NUM_WORKERS,\n","                                            pin_memory=True,\n","                                            persistent_workers=True\n","                                        )\n","        return valid_dataloader"]},{"cell_type":"markdown","metadata":{},"source":["# **Model**"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["class BioModel(nn.Module):\n","    def __init__(self, \n","                 input_dim=142, \n","                 input_dim_embedding=37, \n","                 hidden_dim=128, \n","                 num_filters=32, \n","                 output_dim=3):\n","        super(BioModel, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.input_dim_embedding = input_dim_embedding\n","        self.hidden_dim = hidden_dim\n","        self.num_filters = num_filters\n","        self.output_dim = output_dim\n","\n","        self.embedding = nn.Embedding(num_embeddings=self.input_dim_embedding, embedding_dim=self.hidden_dim, padding_idx=0)\n","        \n","        self.conv1 = nn.Conv1d(in_channels=self.hidden_dim, out_channels=self.num_filters, kernel_size=3, stride=1, padding=0)\n","        self.conv2 = nn.Conv1d(in_channels=self.num_filters, out_channels=self.num_filters*2, kernel_size=3, stride=1, padding=0)\n","        self.conv3 = nn.Conv1d(in_channels=self.num_filters*2, out_channels=self.num_filters*3, kernel_size=3, stride=1, padding=0)\n","        \n","        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n","        \n","        self.fc1 = nn.Linear(self.num_filters*3, 1024)\n","        self.fc2 = nn.Linear(1024, 1024)\n","        self.fc3 = nn.Linear(1024, 512)\n","        \n","        self.dropout = nn.Dropout(0.1)\n","        \n","        self.output = nn.Linear(512, self.output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).permute(0, 2, 1)\n","        \n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        \n","        x = self.global_max_pool(x).squeeze(2)\n","        \n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        \n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        \n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","        \n","        x = self.output(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of parameters: 1717059\n","torch.Size([42, 3])\n"]}],"source":["# check model\n","# test model\n","dummy_model = BioModel()\n","\n","total_params = sum(p.numel() for p in dummy_model.parameters())\n","print(f\"Total number of parameters: {total_params}\")\n","\n","dummy_input = torch.randint(0, 37, (42, 142), dtype=torch.long)\n","output = dummy_model(dummy_input)\n","print(output.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# **Lightning Module**"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class BioModel(LightningModule):\n","    def __init__(self, lr=1e-3, weight_decay=1e-6):\n","        \n","        super(BioModel, self).__init__()\n","       \n","        self.model = BioModel()\n","        self.lr = lr\n","        \n","        self.loss_fn = F.binary_cross_entropy_with_logits()\n","        \n","        self.validation_step_outputs = []\n","        \n","    def forward(self, X):\n","        pred = self.model(X)\n","        return pred\n","    \n","    def configure_optimizers(self):\n","        \n","        # == define optimizer ==\n","        model_optimizer = torch.optim.Adam(\n","            filter(lambda p: p.requires_grad, self.parameters()),\n","            lr=config.LR,\n","            weight_decay=config.WEIGHT_DECAY\n","        )\n","        # == define learning rate scheduler ==\n","        lr_scheduler = CosineAnnealingWarmRestarts(\n","            model_optimizer,\n","            T_0=config.EPOCHS,\n","            T_mult=1,\n","            eta_min=1e-6,\n","            last_epoch=-1\n","        )\n","        \n","        return {\n","            'optimizer': model_optimizer,\n","            'lr_scheduler': {\n","                'scheduler': lr_scheduler,\n","                'interval': 'epoch',\n","                'monitor': 'val_loss',\n","                'frequency': 1\n","            }\n","        }\n","        \n","    def training_step(self, batch, batch_idx):\n","        \n","        X, y = batch.pop('X'), batch.pop('y')\n","        logits = self(X)\n","        train_loss = self.loss_fn(logits, y)\n","        \n","        self.log('train_loss', train_loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        \n","        return train_loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        \n","        X, y = batch.pop('X'), batch.pop('y')\n","        logits = self(X)\n","        valid_loss = self.loss_fn(logits, y)\n","        \n","        self.log('valid_loss', valid_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n","        \n","        self.validation_step_outputs.append({\"valid_loss\":valid_loss})\n","        \n","        return valid_loss\n","    \n","    def train_dataloader(self):\n","        return self._train_dataloader\n","\n","    def validation_dataloader(self):\n","        return self._validation_dataloader\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        return optimizer\n","    \n","    def on_validation_epoch_end(self):\n","        \n","        outputs = self.validation_step_outputs\n","        \n","        # 各iterationごとのlossを平均\n","        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","        self.log(\"val_loss_epoch\", avg_loss, prog_bar=True, logger=True)   \n","        \n","        self.validation_step_outputs.clear()\n","        \n","        return {'val_loss': avg_loss}"]},{"cell_type":"markdown","metadata":{},"source":["# Train & Inference"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def predict_in_batches(model, df):\n","    \n","    model.to(device)\n","    model.eval()\n","    \n","    test_dataset = TensorDataset(torch.tensor(df[FEATURES].values, dtype=torch.int))\n","    test_loader = DataLoader(test_dataset,\n","                             batch_size=config.BATCH_SIZE, \n","                             shuffle=False, \n","                             num_workers=config.NUM_WORKERS, \n","                             pin_memory=True)\n","    \n","    all_preds = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            inputs = batch[0].to(device)\n","            preds = model(inputs)\n","            all_preds.append(preds.cpu().numpy())\n","    \n","    return np.concatenate(all_preds, axis=0)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def run_training(fold_id, folds_list, df):\n","    print('================================================================')\n","    print(f\"==== Running training for fold {fold_id} ====\")\n","    \n","    # == init data module and model ==\n","    train_idx, valid_idx = folds_list[fold_id]\n","    model = BioModel()\n","    datamodule = BioDataModule(df, train_idx, valid_idx)\n","    \n","    # == init callback ==\n","    checkpoint_callback = ModelCheckpoint(monitor='valid_loss',\n","                                          dirpath=paths.MODEL_WEIGHTS_DIR,\n","                                          save_top_k=1,\n","                                          save_last=False,\n","                                          save_weights_only=True,\n","                                          filename=f\"fold_{fold_id}\",\n","                                          mode='min')\n","    early_stop_callback = EarlyStopping(monitor=\"valid_loss\", mode=\"min\", patience=5, verbose=True)\n","    callbacks_to_use = [checkpoint_callback,\n","                        early_stop_callback,\n","                        RichModelSummary(),\n","                        RichProgressBar(),\n","#                         TQDMProgressBar(refresh_rate=1)\n","                       ]\n","\n","    # == init trainer ==\n","    trainer = Trainer(\n","        max_epochs=config.EPOCHS,\n","        callbacks=callbacks_to_use,\n","        accelerator=config.DEVICE,\n","        deterministic=True,\n","        gradient_clip_val=10,\n","        precision='16-mixed' if config.MIXED_PRECISION else 32,\n","        logger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}'),\n","    )\n","    \n","    # == Training ==\n","    trainer.fit(model, datamodule=datamodule)\n","    \n","    # == Prediction by best model==\n","    weights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n","    model.load_state_dict(weights)\n","    \n","    valid_df = datamodule.valid_df\n","    \n","    preds_oof = predict_in_batches(valid_df, model)\n","    y_oof = valid_df[TARGETS].values\n","    score = APS(y_oof, preds_oof, average='micro')\n","    \n","    print(f'fold:{fold} | CV score = {score}')\n","    \n","    preds_test = predict_in_batches(df_test, model)\n","    \n","    del model, datamodule, trainer, preds_oof, y_oof\n","    gc.collect()\n","    \n","    return score, preds_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training\n","# torch.set_float32_matmul_precision('high')\n","\n","all_preds = []\n","score_list = []\n","for fold_id in range(config.FOLDS):\n","    score, preds_test = run_training(fold_id, folds_list, df_train)\n","    \n","    score_list.append(score)\n","    all_preds.append(preds_test)\n","    \n","    # ファイルに書き込み\n","    score_list = [str(loss) for loss in score_list]\n","    with open(paths.OUTPUT_DIR / 'cv_result.txt', 'w') as file:\n","        file.write(', '.join(score_list))\n","        \n","preds = np.mean(all_preds, 0)"]},{"cell_type":"markdown","metadata":{},"source":["# **Submission**"]},{"cell_type":"code","execution_count":47,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'preds' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(paths\u001b[38;5;241m.\u001b[39mDATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m test\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRD4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m[(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRD4\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m test\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHSA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds[(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHSA\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m test\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msEH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds[(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msEH\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m2\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"]}],"source":["test = pd.read_parquet(paths.DATA_DIR / 'test.parquet')\n","test['binds'] = 0\n","test.loc[test['protein_name']=='BRD4', 'binds'] = preds[(test['protein_name']=='BRD4').values, 0]\n","test.loc[test['protein_name']=='HSA', 'binds'] = preds[(test['protein_name']=='HSA').values, 1]\n","test.loc[test['protein_name']=='sEH', 'binds'] = preds[(test['protein_name']=='sEH').values, 2]\n","test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / 'submission.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8006601,"sourceId":67356,"sourceType":"competition"},{"datasetId":4914065,"sourceId":8275617,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
