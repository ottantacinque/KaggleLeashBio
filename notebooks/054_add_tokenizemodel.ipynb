{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leash Bio\n",
    "\n",
    "- positive data多めに使う。残りはrandom sampling\n",
    "- bbごとにsoft labelingしてみる\n",
    "\n",
    "\n",
    "## ref\n",
    "- https://www.kaggle.com/code/yyyu54/pytorch-version-belka-1dcnn-starter-with-all-data\n",
    "- https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = '054'\n",
    "DEBUG = False\n",
    "# data_ratio = 1/5\n",
    "\n",
    "fold_list = [0,1,2,3,4]\n",
    "infer_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (from timm) (2.3.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/site-packages (from timm) (0.18.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/site-packages (from timm) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface_hub->timm) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface_hub->timm) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch->timm) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torchvision->timm) (1.26.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision->timm) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install rdkit\n",
    "# !pip install mordred\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# import polars as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "# seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "\tModelCheckpoint, \n",
    "\tEarlyStopping,\n",
    "\tModelCheckpoint,\n",
    "\tRichModelSummary,\n",
    "\tRichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import timm\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "from funcs.utils import find_latest_ckpt_path, del_old_ckpt_path\n",
    "from funcs.calc_descriptor import calc_rdkit_descriptors, calc_ecfp4_descriptors\n",
    "from funcs.tokenize import tokenize_smiles\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPU(s)\n",
      "pytorch: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def is_kaggle_kernel():\n",
    "\treturn os.path.exists('/kaggle/working')\n",
    "\n",
    "if is_kaggle_kernel():\n",
    "\n",
    "\tBASE_DIR = Path(\"/kaggle\")\n",
    "\tDATA_DIR = BASE_DIR / \"input\"\n",
    "\tOUTPUT_DIR = BASE_DIR / \"working\"\n",
    "\tprint('on kaggle notebook')\n",
    "\n",
    "else:\n",
    "\tBASE_DIR = Path(os.getcwd()) / './../'\n",
    "\tDATA_DIR = BASE_DIR / \"data\"\n",
    "\tOUTPUT_DIR = BASE_DIR / f\"output/exp{exp_no}\"\n",
    "\t\n",
    "# set device\n",
    "if torch.backends.mps.is_available():\n",
    "\tdevice = \"mps\"\n",
    "elif torch.cuda.is_available():    \n",
    "\tdevice = \"cuda\"\n",
    "else:\n",
    "\tdevice = \"cpu\"\n",
    "\t\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "print('pytorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "\tSEED = 2024\n",
    "\t\n",
    "\tPREPROCESS = False\n",
    "\tEPOCHS = 30 #20\n",
    "\tPATIENCE = 5 #20\n",
    "\tBATCH_SIZE = 4096\n",
    "\tNUM_WORKERS = 16\n",
    "\t\n",
    "\tUSE_EMA = False\n",
    "\t\n",
    "\tLR = 1e-3\n",
    "\tWEIGHT_DECAY = 1e-6\n",
    "\tMIXED_PRECISION = True\n",
    "\t\n",
    "\tNUM_FOLDS = 5    \n",
    "\tUSE_NUM_FOLD = 1\n",
    "\t\n",
    "class paths:    \n",
    "\tDATA_DIR = DATA_DIR\n",
    "\tOUTPUT_DIR = OUTPUT_DIR\n",
    "\tMODEL_WEIGHTS_DIR = OUTPUT_DIR / f\"bio-models-exp{exp_no}\"\n",
    "\t\n",
    "\tSHRUNKEN_DATA_DIR = DATA_DIR / \"shrunken-data\"\n",
    "\n",
    "\tTRAIN_PATH = SHRUNKEN_DATA_DIR / \"train.parquet\"\n",
    "\tTEST_PATH = SHRUNKEN_DATA_DIR / \"test.parquet\"\n",
    "\tSUB_PATH = SHRUNKEN_DATA_DIR / \"sub.parquet\"\n",
    "\t\n",
    "\tOUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    }
   ],
   "source": [
    "print('fix seed')\n",
    "\n",
    "def my_seed_everything(seed: int):\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\t\n",
    "# seed_everything(config.SEED, workers=True)\n",
    "my_seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loda Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_cols = ['buildingblock1_smiles', 'buildingblock2_smiles','buildingblock3_smiles', \n",
    "\t\t   'buildingblock1_smiles_scaffold', \"buildingblock2_smiles_scaffold\", \"buildingblock3_smiles_scaffold\",\n",
    "\t\t   'fold']\n",
    "\n",
    "TARGETS = ['binds_BRD4', 'binds_HSA','binds_sEH']\n",
    "\n",
    "df_train = pd.read_parquet(paths.TRAIN_PATH, columns=bb_cols + TARGETS)\n",
    "\n",
    "if DEBUG:\n",
    "\tdf_train = df_train.sample(100000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソフトラベリングを用意する\n",
    "bb1_mean = df_train.groupby('buildingblock1_smiles')[TARGETS].mean()\n",
    "bb2_mean = df_train.groupby('buildingblock2_smiles')[TARGETS].mean()\n",
    "bb3_mean = df_train.groupby('buildingblock3_smiles')[TARGETS].mean()\n",
    "\n",
    "for target in TARGETS:\n",
    "\tdf_train[f'{target}_bb1'] = df_train['buildingblock1_smiles'].map(bb1_mean[target].to_dict())\n",
    "\tdf_train[f'{target}_bb2'] = df_train['buildingblock2_smiles'].map(bb2_mean[target].to_dict())\n",
    "\tdf_train[f'{target}_bb3'] = df_train['buildingblock3_smiles'].map(bb3_mean[target].to_dict())\n",
    "\t\n",
    "df_train['binds_BRD4'] = df_train['binds_BRD4'] + df_train['binds_BRD4_bb1'] + df_train['binds_BRD4_bb2'] + df_train['binds_BRD4_bb3']\n",
    "df_train['binds_HSA'] = df_train['binds_HSA'] + df_train['binds_HSA_bb1'] + df_train['binds_HSA_bb2'] + df_train['binds_HSA_bb3']\n",
    "df_train['binds_sEH'] = df_train['binds_sEH'] + df_train['binds_sEH_bb1'] + df_train['binds_sEH_bb2'] + df_train['binds_sEH_bb3']\n",
    "\n",
    "df_train[TARGETS] = df_train[TARGETS].clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=[f'{target}_bb1' for target in TARGETS], inplace=True)\n",
    "df_train.drop(columns=[f'{target}_bb2' for target in TARGETS], inplace=True)\n",
    "df_train.drop(columns=[f'{target}_bb3' for target in TARGETS], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>buildingblock1_smiles_scaffold</th>\n",
       "      <th>buildingblock2_smiles_scaffold</th>\n",
       "      <th>buildingblock3_smiles_scaffold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>119</td>\n",
       "      <td>9</td>\n",
       "      <td>407</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>131</td>\n",
       "      <td>9</td>\n",
       "      <td>407</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>420</td>\n",
       "      <td>9</td>\n",
       "      <td>407</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>607</td>\n",
       "      <td>9</td>\n",
       "      <td>407</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   buildingblock1_smiles  buildingblock2_smiles  buildingblock3_smiles  \\\n",
       "0                      0                     20                     20   \n",
       "1                      0                     20                    119   \n",
       "2                      0                     20                    131   \n",
       "3                      0                     20                    420   \n",
       "4                      0                     20                    607   \n",
       "\n",
       "   buildingblock1_smiles_scaffold  buildingblock2_smiles_scaffold  \\\n",
       "0                               9                             407   \n",
       "1                               9                             407   \n",
       "2                               9                             407   \n",
       "3                               9                             407   \n",
       "4                               9                             407   \n",
       "\n",
       "   buildingblock3_smiles_scaffold  \n",
       "0                             407  \n",
       "1                             262  \n",
       "2                              16  \n",
       "3                              41  \n",
       "4                             521  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submitt用のデータ\n",
    "df_test = pd.read_parquet(paths.SUB_PATH)\n",
    "df_test.head()\n",
    "\n",
    "# preudolabeling用\n",
    "cols = ['buildingblock1_smiles', 'buildingblock2_smiles',\n",
    "\t   'buildingblock3_smiles', 'buildingblock1_smiles_scaffold',\n",
    "\t   'buildingblock2_smiles_scaffold', 'buildingblock3_smiles_scaffold']\n",
    "df_pseudo = df_test[cols].drop_duplicates().reset_index(drop=True)\n",
    "df_pseudo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変換用辞書を読み込む\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb1_smiles2idx.pickle', mode='rb') as f:\n",
    "\tbb1_smiles2idx = pickle.load(f)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb23_smiles2idx.pickle', mode='rb') as f:\n",
    "\tbb23_smiles2idx = pickle.load(f)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb1_scaffold_smiles2idx.pickle', mode='rb') as f:\n",
    "\tbb1_scaffold_smiles2idx = pickle.load(f)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb23_scaffold_smiles2idx.pickle', mode='rb') as f:\n",
    "\tbb23_scaffold_smiles2idx = pickle.load(f)\n",
    "\t\n",
    "bb1_idx2smiles = {v:k for k,v in bb1_smiles2idx.items()}\n",
    "bb23_idx2smiles = {v:k for k,v in bb23_smiles2idx.items()}\n",
    "bb1_scaffold_idx2smiles = {v:k for k,v in bb1_scaffold_smiles2idx.items()}\n",
    "bb23_scaffold_idx2smiles = {v:k for k,v in bb23_scaffold_smiles2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdkit descriptors\n",
    "df_bb1_rdkit = calc_rdkit_descriptors(bb1_idx2smiles)\n",
    "df_bb23_rdkit = calc_rdkit_descriptors(bb23_idx2smiles)\n",
    "df_bb1_scf_rdkit = calc_rdkit_descriptors(bb1_scaffold_idx2smiles)\n",
    "df_bb23_scf_rdkit = calc_rdkit_descriptors(bb23_scaffold_idx2smiles)\n",
    "\n",
    "# ecfp4 descriptors\n",
    "df_bb1_ecfp4 = calc_ecfp4_descriptors(bb1_idx2smiles)\n",
    "df_bb23_ecfp4 = calc_ecfp4_descriptors(bb23_idx2smiles)\n",
    "df_bb1_scf_ecfp4 = calc_ecfp4_descriptors(bb1_scaffold_idx2smiles)\n",
    "df_bb23_scf_ecfp4 = calc_ecfp4_descriptors(bb23_scaffold_idx2smiles)\n",
    "\n",
    "# tokenize\n",
    "df_bb1_token = tokenize_smiles(bb1_idx2smiles)\n",
    "df_bb23_token = tokenize_smiles(bb23_idx2smiles)\n",
    "df_bb1_scf_token = tokenize_smiles(bb1_scaffold_idx2smiles)\n",
    "df_bb23_scf_token = tokenize_smiles(bb23_scaffold_idx2smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardization(df_list):\n",
    "\t# 複数のdfをまとめて標準化\n",
    "\tdf_all = pd.concat(df_list,axis=0)\n",
    "\tdf_all.drop_duplicates(inplace=True)\n",
    "\tdf_all.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\t\n",
    "\t# 標準偏差が0の列を削除\n",
    "\tdf_all = df_all.loc[:, df_all.std() != 0]\n",
    "\n",
    "\t# standard scaling\n",
    "\tscaler = StandardScaler()\n",
    "\tscaler.fit(df_all)\n",
    "\n",
    "\tstandardized_df_list = []\n",
    "\tfor df_temp in df_list:\n",
    "\t\tdf_temp = df_temp.loc[:, df_all.columns]\n",
    "\t\tdf_temp_std = pd.DataFrame(scaler.transform(df_temp), \n",
    "\t\t\t\t\t\t\t\tindex=df_temp.index, \n",
    "\t\t\t\t\t\t\t\tcolumns=df_temp.columns)\n",
    "\t\tstandardized_df_list.append(df_temp_std)\n",
    "\t\t\n",
    "\treturn standardized_df_list\n",
    "\n",
    "\n",
    "def remove_std0(df_list):\n",
    "\t# 標準偏差が0の列を削除\n",
    "\tdf_all = pd.concat(df_list,axis=0)\n",
    "\tdf_all.drop_duplicates(inplace=True)\n",
    "\tdf_all = df_all.loc[:, df_all.std() != 0]\n",
    "\t\n",
    "\tstandardized_df_list = []\n",
    "\tfor df_temp in df_list:\n",
    "\t\tdf_temp = df_temp.loc[:, df_all.columns]\n",
    "\t\tstandardized_df_list.append(df_temp)\n",
    "\t\t\n",
    "\treturn standardized_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rdkit記述子をまとめて標準化\n",
    "df_list_rdkit = [\n",
    "\t\t\tdf_bb1_rdkit,\n",
    "\t\t\tdf_bb23_rdkit, \n",
    "\t\t\tdf_bb1_scf_rdkit, \n",
    "\t\t\tdf_bb23_scf_rdkit,\n",
    "\t\t\t]\n",
    "df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit = standardization(df_list_rdkit)\n",
    "\t\t\n",
    "# ECFP4記述子をまとめて標準化\n",
    "df_list_ecfp4 = [\n",
    "\t\t\tdf_bb1_ecfp4,\n",
    "\t\t\tdf_bb23_ecfp4, \n",
    "\t\t\tdf_bb1_scf_ecfp4, \n",
    "\t\t\tdf_bb23_scf_ecfp4,\n",
    "\t\t\t]\n",
    "df_bb1_ecfp4,df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4 = remove_std0(df_list_ecfp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 1024 80\n"
     ]
    }
   ],
   "source": [
    "len_rdkit = df_bb1_rdkit.shape[1]\n",
    "len_ecfp4 = df_bb1_ecfp4.shape[1]\n",
    "len_token = df_bb1_token.shape[1]\n",
    "print(len_rdkit, len_ecfp4, len_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset & DataModule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataset(torch.utils.data.Dataset):\n",
    "\t\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tdf: pd.DataFrame,\n",
    "\t\tdf_bb1_desc: pd.DataFrame,\n",
    "\t\tdf_bb23_desc: pd.DataFrame,\n",
    "\t\tdf_bb1_scf_desc: pd.DataFrame,\n",
    "\t\tdf_bb23_scf_desc: pd.DataFrame,\n",
    "\t\tdf_bb1_ecfp: pd.DataFrame,\n",
    "\t\tdf_bb23_ecfp: pd.DataFrame,\n",
    "\t\tdf_bb1_scf_ecfp: pd.DataFrame,\n",
    "\t\tdf_bb23_scf_ecfp: pd.DataFrame,\n",
    "\t\tdf_bb1_token: pd.DataFrame,\n",
    "\t\tdf_bb23_token: pd.DataFrame,\n",
    "\t\tdf_bb1_scf_token: pd.DataFrame,\n",
    "\t\tdf_bb23_scf_token: pd.DataFrame,\n",
    "\t\tmode = 'train',\n",
    "\t\tn_splits = 5,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tassert mode in ['train', 'valid', 'test']\n",
    "\t\tself.mode = mode\n",
    "\t\t\n",
    "\t\tmeta_cols = [\"buildingblock1_smiles\", # 0\n",
    "\t\t\t\t\t \"buildingblock2_smiles\", # 1\n",
    "\t\t\t\t\t \"buildingblock3_smiles\", # 2\n",
    "\t\t\t\t\t \"buildingblock1_smiles_scaffold\", # 3\n",
    "\t\t\t\t\t \"buildingblock2_smiles_scaffold\", # 4\n",
    "\t\t\t\t\t \"buildingblock3_smiles_scaffold\", # 5    \n",
    "\t\t\t\t\t ]\n",
    "\t\tif (self.mode == 'train') or (self.mode == 'valid'):\n",
    "\t\t\tmeta_cols += TARGETS\n",
    "\t\t\t\n",
    "\t\tdf = df[meta_cols]\n",
    "\t\t\n",
    "\t\t# 5分割する\n",
    "\t\tif self.mode == 'train':\n",
    "\t\t\tself.df_array = self.split_negative_sample(df, n_splits) # (index=5, num_sample, len(meta_cols))\n",
    "\t\telse:\n",
    "\t\t\tself.df_array = np.array([df.values]) # (1, num_sample, len(meta_cols))\n",
    "\t\tself.choices = [i for i in range(n_splits)]\n",
    "\n",
    "\t\tself.bb1_desc = df_bb1_desc.values\n",
    "\t\tself.bb23_desc = df_bb23_desc.values\n",
    "\t\tself.bb1_scf_desc = df_bb1_scf_desc.values\n",
    "\t\tself.bb23_scf_desc = df_bb23_scf_desc.values\n",
    "\t\t\n",
    "\t\tself.bb1_ecfp = df_bb1_ecfp.values\n",
    "\t\tself.bb23_ecfp = df_bb23_ecfp.values\n",
    "\t\tself.bb1_scf_ecfp = df_bb1_scf_ecfp.values\n",
    "\t\tself.bb23_scf_ecfp = df_bb23_scf_ecfp.values\n",
    "\n",
    "\t\tself.bb1_token = df_bb1_token.values\n",
    "\t\tself.bb23_token = df_bb23_token.values\n",
    "\t\tself.bb1_scf_token = df_bb1_scf_token.values\n",
    "\t\tself.bb23_scf_token = df_bb23_scf_token.values\n",
    "\t\t\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.df_array.shape[1]\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t\n",
    "\t\tif self.mode == 'train':\n",
    "\t\t\tselected_array = self.df_array[random.choice(self.choices)]\n",
    "\t\telse:\n",
    "\t\t\tselected_array = self.df_array[0]\n",
    "\n",
    "\t\trow = selected_array[index, :]\n",
    "\t\t\n",
    "\t\tbb1_idx = int(row[0])\n",
    "\t\tbb2_idx = int(row[1])\n",
    "\t\tbb3_idx = int(row[2])\n",
    "\t\tbb1_scf_idx = int(row[3])\n",
    "\t\tbb2_scf_idx = int(row[4])\n",
    "\t\tbb3_scf_idx = int(row[5])\n",
    "\t\t\n",
    "\t\tif self.mode == 'train':\n",
    "\t\t\tif np.random.rand() < 0.5:\n",
    "\t\t\t\tbb2_idx, bb3_idx, bb2_scf_idx, bb3_scf_idx = bb3_idx, bb2_idx, bb3_scf_idx, bb2_scf_idx\n",
    "\n",
    "\t\tbb1_desc = self.bb1_desc[bb1_idx, :]\n",
    "\t\tbb2_desc = self.bb23_desc[bb2_idx, :]\n",
    "\t\tbb3_desc = self.bb23_desc[bb3_idx, :]\n",
    "\t\tbb1_scf_desc = self.bb1_scf_desc[bb1_scf_idx, :]\n",
    "\t\tbb2_scf_desc = self.bb23_scf_desc[bb2_scf_idx, :]\n",
    "\t\tbb3_scf_desc = self.bb23_scf_desc[bb3_scf_idx, :]\n",
    "\t\t\n",
    "\t\tbb1_ecfp = self.bb1_ecfp[bb1_idx, :]\n",
    "\t\tbb2_ecfp = self.bb23_ecfp[bb2_idx, :]\n",
    "\t\tbb3_ecfp = self.bb23_ecfp[bb3_idx, :]\n",
    "\t\tbb1_scf_ecfp = self.bb1_scf_ecfp[bb1_scf_idx, :]\n",
    "\t\tbb2_scf_ecfp = self.bb23_scf_ecfp[bb2_scf_idx, :]\n",
    "\t\tbb3_scf_ecfp = self.bb23_scf_ecfp[bb3_scf_idx, :]\n",
    "\n",
    "\t\tbb1_token = self.bb1_token[bb1_idx, :]\n",
    "\t\tbb2_token = self.bb23_token[bb2_idx, :]\n",
    "\t\tbb3_token = self.bb23_token[bb3_idx, :]\n",
    "\t\tbb1_scf_token = self.bb1_scf_token[bb1_scf_idx, :]\n",
    "\t\tbb2_scf_token = self.bb23_scf_token[bb2_scf_idx, :]\n",
    "\t\tbb3_scf_token = self.bb23_scf_token[bb3_scf_idx, :]\n",
    "\t\t\n",
    "\t\tX = np.concatenate([bb1_desc, bb2_desc, bb3_desc, bb1_scf_desc, bb2_scf_desc, bb3_scf_desc,\n",
    "\t\t\t\t\t\t\t bb1_ecfp, bb2_ecfp, bb3_ecfp, bb1_scf_ecfp, bb2_scf_ecfp, bb3_scf_ecfp,\n",
    "\t\t\t\t\t\t\t bb1_token, bb2_token, bb3_token, bb1_scf_token, bb2_scf_token, bb3_scf_token,\n",
    "\t\t\t\t\t\t\t ])\n",
    "\t\t\n",
    "\t\tif (self.mode == 'train') or (self.mode == 'valid'):\n",
    "\t\t\ty = row[-3:]\n",
    "\t\telse:\n",
    "\t\t\ty = np.zeros(3)\n",
    "\t\t\n",
    "\t\toutput = {\n",
    "\t\t\t'X': torch.tensor(X, dtype=torch.float32),\n",
    "\t\t\t'y': torch.tensor(y, dtype=torch.float16)\n",
    "\t\t}        \n",
    "\t\treturn output\n",
    "\t\n",
    "\t\n",
    "\tdef split_negative_sample(self, df, n_splits=5):\n",
    "\t\t# 5分割されたnegative sampleがとpositive sampleが結合したarrayを作成する\n",
    "\t\t\n",
    "\t\tdf_positive = df[(df[TARGETS]==1).any(axis=1)]\n",
    "\t\tdf_negative = df[~(df[TARGETS]==1).any(axis=1)]\n",
    "\n",
    "\t\tpositive_array = df_positive.values\n",
    "\t\tnegative_array = df_negative.sample(frac=1, random_state=config.SEED).values\n",
    "\n",
    "\t\tsplit_size = negative_array.shape[0] // n_splits\n",
    "\n",
    "\t\tsplits = []\n",
    "\t\tfor i in range(n_splits):\n",
    "\t\t\tstart = i * split_size\n",
    "\t\t\tend = start + split_size\n",
    "\t\t\tsplited_array = np.concatenate([negative_array[start:end], positive_array])\n",
    "\t\t\tsplits.append(splited_array)\n",
    "\n",
    "\t\t# n_splitsつに分割\n",
    "\t\tsplits = np.array(splits)\n",
    "\t\t\n",
    "\t\treturn splits\n",
    "\n",
    "\t\n",
    "\t# def augment(self, \n",
    "\t#             bb2_desc, bb3_desc, bb2_scf_desc, bb3_scf_desc,\n",
    "\t#             bb2_ecfp, bb3_ecfp, bb2_scf_ecfp, bb3_scf_ecfp):\n",
    "\t#     \"\"\"0.5の確率でx2とx3を入れ替えるaugmentation\"\"\"\n",
    "\t#     if np.random.rand() < 0.5:\n",
    "\t#         bb2_desc, bb3_desc = bb3_desc, bb2_desc\n",
    "\t#         bb2_scf_desc, bb3_scf_desc = bb3_scf_desc, bb2_scf_desc\n",
    "\t#         bb2_ecfp, bb3_ecfp = bb3_ecfp, bb2_ecfp\n",
    "\t#         bb2_scf_ecfp, bb3_scf_ecfp = bb3_scf_ecfp, bb2_scf_ecfp\n",
    "\t#     return bb2_desc, bb3_desc, bb2_scf_desc, bb3_scf_desc, bb2_ecfp, bb3_ecfp, bb2_scf_ecfp, bb3_scf_ecfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7746])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Check Dataset\n",
    "# if DEBUG:\n",
    "dataset = BioDataset(df_train, \n",
    "\t\t\t\t\t\tdf_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "\t\t\t\t\t\tdf_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "\t\t\t\t\t\tdf_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "\t\t\t\t\t\tmode='train')\n",
    "X = dataset[0]['X']\n",
    "y = dataset[0]['y']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning data module\n",
    "class BioDataModule(LightningDataModule):\n",
    "\tdef __init__(self, df_train, fold_id):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.train_df = df_train[df_train['fold'] != fold_id]\n",
    "\t\tself.valid_df = df_train[df_train['fold'] == fold_id]\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\ttrain_dataset = BioDataset(self.train_df, \n",
    "\t\t\t\t\t\t\t\tdf_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\tdf_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\tdf_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "\t\t\t\t\t\t\t\t   mode='train')\n",
    "\t\ttrain_dataloader = torch.utils.data.DataLoader(\n",
    "\t\t\t\t\t\t\t\ttrain_dataset,\n",
    "\t\t\t\t\t\t\t\tbatch_size=config.BATCH_SIZE,\n",
    "\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\tnum_workers=config.NUM_WORKERS,\n",
    "\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\tpersistent_workers=True,\n",
    "\t\t\t\t\t\t\t\tdrop_last=True,\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\treturn train_dataloader\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\tvalid_dataset = BioDataset(self.valid_df, \n",
    "\t\t\t\t\t\t\t\tdf_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\tdf_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\tdf_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "\t\t\t\t\t\t\t\t   mode='valid')\n",
    "\t\tvalid_dataloader = torch.utils.data.DataLoader(\n",
    "\t\t\t\t\t\t\t\t\t\t\tvalid_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_size=config.BATCH_SIZE,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=config.NUM_WORKERS,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpersistent_workers=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdrop_last=False,\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\treturn valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (3687949982.py, line 133)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 133\u001b[0;36m\u001b[0m\n\u001b[0;31m    bb1_token = self.embedding(bb1_token).permute(0, 2, 1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "class BioModel(nn.Module):\n",
    "\tdef __init__(self, \n",
    "\t\t\t\t input_len1,\n",
    "\t\t\t\t input_len2,\n",
    "\t\t\t\t input_len3,\n",
    "\t\t\t\t output_dim=3):\n",
    "\t\tsuper(BioModel, self).__init__()\n",
    "\t\t\n",
    "\t\tself.input_len1 = input_len1\n",
    "\t\tself.input_len2 = input_len2\n",
    "\t\tself.input_len3 = input_len3\n",
    "\t\tself.output_dim = output_dim\n",
    "\t\t\n",
    "\t\t# それぞれの記述子のFC（desc1）\n",
    "\t\tself.feature_extractor_bb1_desc1 = self._make_feature_extractor(input_len1, 128)\n",
    "\t\tself.feature_extractor_bb23_desc1 = self._make_feature_extractor(input_len1, 128)\n",
    "\t\tself.feature_extractor_bb1scf_desc1 = self._make_feature_extractor(input_len1, 128)\n",
    "#         self.feature_extractor_bb23scf_desc1 = self._make_feature_extractor(input_len1, 128)\n",
    "\t\t\n",
    "\t\t # それぞれの記述子のFC（desc2）\n",
    "\t\tself.feature_extractor_bb1_desc2 = self._make_feature_extractor(input_len2,128)\n",
    "\t\tself.feature_extractor_bb23_desc2 = self._make_feature_extractor(input_len2,128)\n",
    "\t\tself.feature_extractor_bb1scf_desc2 = self._make_feature_extractor(input_len2,128)\n",
    "#         self.feature_extractor_bb23scf_desc2 = self._make_feature_extractor(input_len2,128)\n",
    "\t\t\t\n",
    "\t\t# それぞれのBBのFC\n",
    "\t\tself.feature_extractor_bb1 = self._make_feature_extractor(128*2, 324)\n",
    "\t\tself.feature_extractor_bb23 = self._make_feature_extractor(128*2, 324)\n",
    "\t\tself.feature_extractor_bb1scf = self._make_feature_extractor(128*2, 324)\n",
    "#         self.feature_extractor_bb23scf = self._make_feature_extractor(128*2, 324)\n",
    "\n",
    "\t\t# token feature extractor\n",
    "        self.embedding = nn.Embedding(num_embeddings=37, embedding_dim=128, padding_idx=0)\n",
    "\t\tself.feature_extractor_bb1_token = self._make_token_feature_extractor()\n",
    "\t\tself.feature_extractor_bb23_token = self._make_token_feature_extractor()\n",
    "\t\tself.feature_extractor_bb1scf_token = self._make_token_feature_extractor()\n",
    "\n",
    "\t\t# head\n",
    "\t\tself.head = nn.Sequential(\n",
    "\t\t\tnn.Linear(324*4 + 32*3*4, 1024),\n",
    "\t\t\tnn.BatchNorm1d(1024),\n",
    "\t\t\tnn.Dropout(0.1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(1024, 512),\n",
    "\t\t\tnn.BatchNorm1d(512),\n",
    "\t\t\tnn.Dropout(0.1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(512, 3),\n",
    "\t\t)\n",
    "\t\t\n",
    "\tdef _make_feature_extractor(self, input_len, output_len):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\t\t\t\t\tnn.Linear(input_len, output_len),\n",
    "\t\t\t\t\t\t\tnn.BatchNorm1d(output_len),\n",
    "\t\t\t\t\t\t\tnn.Dropout(0.1),\n",
    "\t\t\t\t\t\t\tnn.ReLU(),\n",
    "\t\t\t\t\t\t)\n",
    "\t\n",
    "\tdef _make_token_feature_extractor(self, hidden_dim=128, num_filters=32):\n",
    "\t\treturn nn.Sequential(\n",
    "\t\t\tnn.Conv1d(in_channels=hidden_dim, out_channels=num_filters, kernel_size=3, stride=1, padding=0),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv1d(in_channels=num_filters, out_channels=num_filters*2, kernel_size=3, stride=1, padding=0),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv1d(in_channels=num_filters*2, out_channels=num_filters*3, kernel_size=3, stride=1, padding=0),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.AdaptiveMaxPool1d(1)\n",
    "\t\t)\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tdesc1 = x[:, :self.input_len1*6]\n",
    "\t\tdesc2 = x[:, self.input_len1*6:self.input_len1*6+self.input_len2*6]\n",
    "\t\ttoken = x[:, self.input_len1*6+self.input_len2*6:]\n",
    "\t\t\n",
    "\t\t# BBの特徴量をとりだす\n",
    "\t\tbb1_desc1 = desc1[:, :self.input_len1]\n",
    "\t\tbb2_desc1 = desc1[:, self.input_len1:self.input_len1*2]\n",
    "\t\tbb3_desc1 = desc1[:, self.input_len1*2:self.input_len1*3]\n",
    "\t\tbb1_scf_desc1 = desc1[:, self.input_len1*3:self.input_len1*4]\n",
    "#         bb2_scf_desc1 = desc1[:, self.input_len1*4:self.input_len1*5]\n",
    "#         bb3_scf_desc1 = desc1[:, self.input_len1*5:]\n",
    "\t\t\n",
    "\t\tbb1_desc2 = desc2[:, :self.input_len2]\n",
    "\t\tbb2_desc2 = desc2[:, self.input_len2:self.input_len2*2]\n",
    "\t\tbb3_desc2 = desc2[:, self.input_len2*2:self.input_len2*3]\n",
    "\t\tbb1_scf_desc2 = desc2[:, self.input_len2*3:self.input_len2*4]\n",
    "#         bb2_scf_desc2 = desc2[:, self.input_len2*4:self.input_len2*5]\n",
    "#         bb3_scf_desc2 = desc2[:, self.input_len2*5:]\n",
    "\n",
    "\t\tbb1_token = token[:, :self.input_len3]\n",
    "\t\tbb2_token = token[:, self.input_len3:self.input_len3*2]\n",
    "\t\tbb3_token = token[:, self.input_len3*2:self.input_len3*3]\n",
    "\t\tbb1_scf_token = token[:, self.input_len3*3:self.input_len3*4]\n",
    "#         bb2_scf_desc2 = desc2[:, self.input_len2*4:self.input_len2*5]\n",
    "#         bb3_scf_desc2 = desc2[:, self.input_len2*5:]\n",
    "\t\t\n",
    "\t\t# 各BB, 各記述子のFC\n",
    "\t\tbb1_desc1 = self.feature_extractor_bb1_desc1(bb1_desc1)\n",
    "\t\tbb2_desc1 = self.feature_extractor_bb23_desc1(bb2_desc1)\n",
    "\t\tbb3_desc1 = self.feature_extractor_bb23_desc1(bb3_desc1)\n",
    "\t\tbb1_scf_desc1 = self.feature_extractor_bb1scf_desc1(bb1_scf_desc1)\n",
    "#         bb2_scf_desc1 = self.feature_extractor_bb23scf_desc1(bb2_scf_desc1)\n",
    "#         bb3_scf_desc1 = self.feature_extractor_bb23scf_desc1(bb3_scf_desc1)\n",
    "\t\t\n",
    "\t\tbb1_desc2 = self.feature_extractor_bb1_desc2(bb1_desc2)\n",
    "\t\tbb2_desc2 = self.feature_extractor_bb23_desc2(bb2_desc2)\n",
    "\t\tbb3_desc2 = self.feature_extractor_bb23_desc2(bb3_desc2)\n",
    "\t\tbb1_scf_desc2 = self.feature_extractor_bb1scf_desc2(bb1_scf_desc2)\n",
    "#         bb2_scf_desc2 = self.feature_extractor_bb23scf_desc2(bb2_scf_desc2)\n",
    "#         bb3_scf_desc2 = self.feature_extractor_bb23scf_desc2(bb3_scf_desc2)\n",
    "\t\t\n",
    "\t\t# desc1(rdkit)とdesc2(ecfp4)をconcat\n",
    "\t\tbb1 = torch.cat([bb1_desc1, bb1_desc2], dim=1)\n",
    "\t\tbb2 = torch.cat([bb2_desc1, bb2_desc2], dim=1)\n",
    "\t\tbb3 = torch.cat([bb3_desc1, bb3_desc2], dim=1)\n",
    "\t\tbb1scf = torch.cat([bb1_scf_desc1, bb1_scf_desc2], dim=1)\n",
    "#         bb2scf = torch.cat([bb2_scf_desc1, bb2_scf_desc2], dim=1)\n",
    "#         bb3scf = torch.cat([bb3_scf_desc1, bb3_scf_desc2], dim=1)\n",
    "\t\t\n",
    "\t\t# 各BBのFC\n",
    "\t\tbb1 = self.feature_extractor_bb1(bb1)\n",
    "\t\tbb2 = self.feature_extractor_bb23(bb2)\n",
    "\t\tbb3 = self.feature_extractor_bb23(bb3)\n",
    "\t\t\n",
    "\t\t# ↓もともとここにミスがあったかも？\n",
    "\t\tbb1scf = self.feature_extractor_bb1scf(bb1scf)\n",
    "#         bb2scf = self.feature_extractor_bb23scf(bb2scf)\n",
    "#         bb3scf = self.feature_extractor_bb23scf(bb3scf)\n",
    "\n",
    "\n",
    "\t\t# token feature extractor ############\n",
    "  \t\tbb1_token = self.embedding(bb1_token).permute(0, 2, 1) \n",
    "  \t\tbb2_token = self.embedding(bb2_token).permute(0, 2, 1) \n",
    "  \t\tbb3_token = self.embedding(bb3_token).permute(0, 2, 1) \n",
    "  \t\tbb1_scf_token = self.embedding(bb1_scf_token).permute(0, 2, 1) \n",
    "    \n",
    "\t\t# feature extractor\n",
    "        bb1_token = self.feature_extractor_bb1_token(bb1_token).squeeze(-1)\n",
    "        bb2_token = self.feature_extractor_bb23_token(bb2_token).squeeze(-1)\n",
    "        bb3_token = self.feature_extractor_bb23_token(bb3_token).squeeze(-1)\n",
    "        bb1_scf_token = self.feature_extractor_bb1scf_token(bb1_scf_token).squeeze(-1)\n",
    "        \n",
    "\t\t# 全てconcat\n",
    "\t\tX = torch.cat([bb1, bb2, bb3, bb1scf, \n",
    "#                        bb2scf, bb3scf\n",
    "\t\t\t\t\t\tbb1_token, bb2_token, bb3_token, bb1_scf_token,\n",
    "\t\t\t\t\t  ], dim=1)\n",
    "\t\t\n",
    "\t\toutput = self.head(X)\n",
    "\t\t\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model\n",
    "if DEBUG:\n",
    "\tdummy_model = BioModel(input_len1=len_rdkit, input_len2=len_ecfp4)\n",
    "\ttotal_params = sum(p.numel() for p in dummy_model.parameters())\n",
    "\tprint(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "\tdummy_input = torch.rand((64, (len_rdkit+len_ecfp4)*6), dtype=torch.float32)\n",
    "\toutput = dummy_model(dummy_input)\n",
    "\tprint(output.shape)\n",
    "\t# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(y_preds, y_true):\n",
    "\t\n",
    "\ty_true[y_true < 1] = 0\n",
    "\t\n",
    "\tscore_BRD4 = APS(y_true[:,0], y_preds[:,0])\n",
    "\tscore_HSA = APS(y_true[:,1], y_preds[:,1])\n",
    "\tscore_sEH = APS(y_true[:,2], y_preds[:,2])\n",
    "\tscore = (score_BRD4 + score_HSA + score_sEH) / 3\n",
    "\t\n",
    "\treturn score_BRD4, score_HSA, score_sEH, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModule(LightningModule):\n",
    "\tdef __init__(self):\n",
    "\t\t\n",
    "\t\tsuper(BioModule, self).__init__()\n",
    "\t   \n",
    "\t\tself.model = BioModel(input_len1=len_rdkit, input_len2=len_ecfp4)\n",
    "\t\t\n",
    "\t\tif config.USE_EMA:\n",
    "\t\t\tself.ema = ModelEmaV2(self.model, decay=0.999)\n",
    "\t\t\n",
    "\t\tself.validation_step_outputs = []\n",
    "\t\tself.loss_func = nn.BCEWithLogitsLoss()\n",
    "\t\t\n",
    "\tdef forward(self, X):\n",
    "\t\tpred = self.model(X)\n",
    "\t\treturn pred\n",
    "\t\n",
    "\tdef configure_optimizers(self):\n",
    "\t\t\n",
    "\t\t# == define optimizer ==\n",
    "\t\tmodel_optimizer = torch.optim.Adam(\n",
    "\t\t\tfilter(lambda p: p.requires_grad, self.parameters()),\n",
    "\t\t\tlr=config.LR,\n",
    "\t\t\tweight_decay=config.WEIGHT_DECAY\n",
    "\t\t)\n",
    "\t\t# == define learning rate scheduler ==\n",
    "\t\tlr_scheduler = CosineAnnealingWarmRestarts(\n",
    "\t\t\tmodel_optimizer,\n",
    "\t\t\tT_0=config.EPOCHS,\n",
    "\t\t\tT_mult=1,\n",
    "\t\t\teta_min=1e-6,\n",
    "\t\t\tlast_epoch=-1\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t'optimizer': model_optimizer,\n",
    "\t\t\t'lr_scheduler': {\n",
    "\t\t\t\t'scheduler': lr_scheduler,\n",
    "\t\t\t\t'interval': 'epoch',\n",
    "\t\t\t\t'monitor': 'valid_loss_epoch',\n",
    "\t\t\t\t'frequency': 1\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\t\n",
    "\t\tX, y = batch.pop('X'), batch.pop('y')\n",
    "\t\tlogits = self(X)\n",
    "\t\ttrain_loss = self.loss_func(logits, y)\n",
    "\t\t\n",
    "\t\tself.log('train_loss', train_loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "\t\t\n",
    "\t\t# EMAの更新\n",
    "\t\tif config.USE_EMA:\n",
    "\t\t\tself.ema.update(self.model)\n",
    "\t\t\n",
    "\t\treturn train_loss\n",
    "\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\t\n",
    "\t\tX, y = batch.pop('X'), batch.pop('y')\n",
    "\t\tlogits = self(X)\n",
    "\t\tpreds = torch.sigmoid(logits)\n",
    "\t\t\n",
    "\t\tvalid_loss = self.loss_func(logits, y)\n",
    "\t\t\n",
    "\t\tself.log('valid_loss', valid_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=X.size(0))\n",
    "\t\t\n",
    "\t\tself.validation_step_outputs.append({\"valid_loss\":valid_loss, \"preds\":preds, \"targets\":y})\n",
    "\t\t\n",
    "\t\treturn valid_loss\n",
    "\n",
    "\t\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn self._train_dataloader\n",
    "\n",
    "\tdef validation_dataloader(self):\n",
    "\t\treturn self._validation_dataloader\n",
    "\t\n",
    "\tdef calc_score(self, y_preds, y_true):\n",
    "\t\treturn calc_score(y_preds, y_true)\n",
    "\n",
    "\t\n",
    "\tdef on_validation_epoch_end(self):\n",
    "\t\t\n",
    "\t\toutputs = self.validation_step_outputs\n",
    "\t\t\n",
    "\t\t# 各iterationごとのlossを平均\n",
    "\t\tavg_loss = torch.stack([x['valid_loss'] for x in outputs]).mean()\n",
    "\t\tself.log(\"valid_loss_epoch\", avg_loss, prog_bar=True, logger=True)\n",
    "\t\t\n",
    "\t\t# scoreを計算\n",
    "\t\ty_preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n",
    "\t\ty_true = torch.cat([x['targets'] for x in outputs]).detach().cpu().numpy()\n",
    "\t\t\n",
    "\t\tscore = self.calc_score(y_preds, y_true)[-1]\n",
    "\t\tself.log(\"valid_score\", score, prog_bar=True, logger=True)\n",
    "\t\t\n",
    "\t\tself.validation_step_outputs.clear()\n",
    "\t\t\n",
    "\t\treturn {'valid_loss_epoch': avg_loss, \"valid_score\":score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, df, \n",
    "\t\t\t\t\t   df_bb1_1, df_bb2_1, df_bb3_1, df_bb1_scf_1, \n",
    "\t\t\t\t\t   df_bb1_2, df_bb2_2, df_bb3_2, df_bb1_scf_2, \n",
    "\t\t\t\t\t   mode):\n",
    "\t\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\tdataset = BioDataset(df, \n",
    "\t\t\t\t\t\t  df_bb1_1, df_bb2_1, df_bb3_1, df_bb1_scf_1, \n",
    "\t\t\t\t\t\t df_bb1_2, df_bb2_2, df_bb3_2, df_bb1_scf_2, \n",
    "\t\t\t\t\t\t mode=mode)\n",
    "\tdataloader = torch.utils.data.DataLoader(\n",
    "\t\t\t\t\t\t\t\t\t\tdataset,\n",
    "\t\t\t\t\t\t\t\t\t\tbatch_size=config.BATCH_SIZE,\n",
    "\t\t\t\t\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\t\t\t\t\tnum_workers=config.NUM_WORKERS,\n",
    "\t\t\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\t\t\tpersistent_workers=True,\n",
    "\t\t\t\t\t\t\t\t\t\tdrop_last=False,\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tall_preds = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\tinputs = batch['X'].to(device)\n",
    "\t\t\tlogits = model(inputs)\n",
    "\t\t\tpreds = torch.sigmoid(logits)\n",
    "\t\t\tall_preds.append(preds.cpu().numpy())\n",
    "\t\n",
    "\treturn np.concatenate(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, df):\n",
    "\tprint(f\"======== Running training for fold {fold_id} =============\")\n",
    "\t\n",
    "\t# == init data module and model ==\n",
    "\tmodel = BioModule()\n",
    "\tdatamodule = BioDataModule(df, fold_id)\n",
    "\t\n",
    "\t# == init callback ==\n",
    "\tcheckpoint_callback = ModelCheckpoint(\n",
    "\t\t\t\t\t\t\t\t\t\tmonitor='valid_score',\n",
    "\t\t\t\t\t\t\t\t\t\t  dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "\t\t\t\t\t\t\t\t\t\t  save_top_k=1,\n",
    "\t\t\t\t\t\t\t\t\t\t  save_last=False,\n",
    "\t\t\t\t\t\t\t\t\t\t  save_weights_only=True,\n",
    "\t\t\t\t\t\t\t\t\t\t  filename=f\"fold_{fold_id}\",\n",
    "\t\t\t\t\t\t\t\t\t\t  mode='max'\n",
    "\t\t\t\t\t\t\t\t\t\t  )\n",
    "\tearly_stop_callback = EarlyStopping(\n",
    "\t\tmonitor='valid_score',\n",
    "\t\tmode=\"max\", \n",
    "\t\tpatience=config.PATIENCE,\n",
    "\t\tverbose=True\n",
    "\t\t)\n",
    "\tcallbacks_to_use = [checkpoint_callback,\n",
    "\t\t\t\t\t\tearly_stop_callback,\n",
    "\t\t\t\t\t\tRichModelSummary(),\n",
    "\t\t\t\t\t\tRichProgressBar(),\n",
    "\t\t\t\t\t   ]\n",
    "\n",
    "\t# == init trainer ==\n",
    "\ttrainer = Trainer(\n",
    "\t\tmax_epochs=config.EPOCHS,\n",
    "\t\tcallbacks=callbacks_to_use,\n",
    "\t\taccelerator=device,\n",
    "\t\tdevices=-1,  # 全ての利用可能なGPUを使用\n",
    "\t\tdeterministic=False,\n",
    "\t\tprecision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "\t\tlogger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}'),\n",
    "\t)\n",
    "\t\n",
    "\n",
    "\t# == Training ==\n",
    "\ttrainer.fit(model, datamodule=datamodule)\n",
    "\t# weights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "\n",
    "\n",
    "def run_inference(fold_id, df):\n",
    "\tprint(f\"======== Inference for fold {fold_id} =============\")\n",
    "\n",
    "\t# == init data module and model ==\n",
    "\tmodel = BioModule()\n",
    "\tdatamodule = BioDataModule(df, fold_id)\n",
    "\n",
    "\t# infer only\n",
    "\tckpt_path = find_latest_ckpt_path(fold_id, paths.MODEL_WEIGHTS_DIR) \n",
    "\tweights = torch.load(ckpt_path)['state_dict']\n",
    "\n",
    "\tmodel.load_state_dict(weights)\n",
    "\t\n",
    "\tvalid_df = datamodule.valid_df\n",
    "\t\n",
    "\tpreds_oof = predict_in_batches(model, valid_df, \n",
    "\t\t\t\t\t\t\t\t  df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\tdf_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\tmode='valid')\n",
    "\ty_oof = valid_df[TARGETS].values\n",
    "\t\n",
    "\tscore_BRD4, score_HSA, score_sEH, score = calc_score(preds_oof, y_oof)\n",
    "\t\n",
    "\tvalid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
    "\t\n",
    "\tprint(f'fold:{fold_id} | CV score = {score}')\n",
    "\t\n",
    "\tdf_test_temp = df_test.drop(['id'], axis=1)\n",
    "\tpreds_test = predict_in_batches(model, df_test_temp, \n",
    "\t\t\t\t\t\t\t\tdf_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\tdf_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\tmode='test')\n",
    "\t\n",
    "\tdel model, datamodule, trainer, preds_oof, y_oof\n",
    "\tgc.collect()\n",
    "\t\n",
    "\tscore_dict = {\n",
    "\t\t'BRD4':score_BRD4,\n",
    "\t\t\"HSA\":score_HSA,\n",
    "\t\t\"sEH\":score_sEH,\n",
    "\t\t\"all\":score\n",
    "\t}\n",
    "\t\n",
    "\treturn preds_test, score_dict, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Running training for fold 0 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp052/bio-models-exp052/fold_0.ckpt\n",
      "fold:0 | CV score = 0.27764881752844406\n",
      "======== Running training for fold 1 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp052/bio-models-exp052/fold_1.ckpt\n",
      "fold:1 | CV score = 0.32221995467273573\n",
      "======== Running training for fold 2 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp052/bio-models-exp052/fold_2.ckpt\n",
      "fold:3 | CV score = 0.29707423903927216\n",
      "======== Running training for fold 4 =============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint file: /home/working/notebooks/../output/exp052/bio-models-exp052/fold_4.ckpt\n",
      "fold:4 | CV score = 0.35050494520492914\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "score_list = []\n",
    "score_list_BRD4 = []\n",
    "score_list_HSA = []\n",
    "score_list_sEH = []\n",
    "\n",
    "def save_list_by_text(score_list, filename):\n",
    "\t# ファイルに書き込み\n",
    "\tscore_list_txt = [str(loss) for loss in score_list]\n",
    "\twith open(paths.OUTPUT_DIR / f'{filename}.txt', 'w') as file:\n",
    "\t\tfile.write(', '.join(score_list_txt))\n",
    "\n",
    "# training\n",
    "if not infer_only:\n",
    "\tfor fold_id in fold_list:\n",
    "\t\trun_training(fold_id, df_train)\n",
    "\n",
    "# inference\n",
    "for fold_id in [0,1,2,3,4]:\n",
    "\tpreds_test, score_dict, df_oof = run_inference(fold_id, df_train)\n",
    "\t\n",
    "\t# save score\n",
    "\tscore_list_BRD4.append(score_dict['BRD4'])\n",
    "\tscore_list_HSA.append(score_dict['HSA'])\n",
    "\tscore_list_sEH.append(score_dict['sEH'])\n",
    "\tscore_list.append(score_dict['all'])\n",
    "\t\n",
    "\tsave_list_by_text(score_list, 'cv_all')\n",
    "\tsave_list_by_text(score_list_BRD4, 'cv_BRD4')\n",
    "\tsave_list_by_text(score_list_HSA, 'cv_HSA')\n",
    "\tsave_list_by_text(score_list_sEH, 'cv_sEH')\n",
    "\t\n",
    "\t# save preds（foldごと）\n",
    "\tall_preds.append(preds_test) \n",
    "\t\n",
    "\tdf_oof.to_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "\t\n",
    "\tdel df_oof\n",
    "\tgc.collect()\n",
    "\t\n",
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "\tdf_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "\tdf_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "\n",
    "df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")\n",
    "\n",
    "# 古いckpt pathを削除\n",
    "for fold in range(0, 5): \n",
    "\tdel_old_ckpt_path(fold, paths.MODEL_WEIGHTS_DIR)\n",
    "\toof_path = paths.OUTPUT_DIR / f'oof_fold_{fold}.parquet'\n",
    "\toof_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(all_preds, 0)\n",
    "\n",
    "df_test['binds'] = 0\n",
    "df_test.loc[df_test['protein_name']=='BRD4', 'binds'] = preds[df_test['protein_name']=='BRD4', 0]\n",
    "df_test.loc[df_test['protein_name']=='HSA', 'binds'] = preds[df_test['protein_name']=='HSA', 1]\n",
    "df_test.loc[df_test['protein_name']=='sEH', 'binds'] = preds[df_test['protein_name']=='sEH', 2]\n",
    "df_test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_1st.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sharedbb, nonsharedbb\n",
    "df_sub = pd.read_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_1st.csv')\n",
    "\n",
    "# load parquet dict data\n",
    "with open(paths.DATA_DIR / 'my-data/test_id_dict.p', 'rb') as file:\n",
    "\ttest_id_dict = pickle.load(file)\n",
    "\t\n",
    "df_shared = df_sub.copy()\n",
    "df_non_shared = df_sub.copy()\n",
    "\n",
    "df_shared.loc[~df_shared['id'].isin(test_id_dict['shared_bb']), 'binds'] = 0\n",
    "df_non_shared.loc[~df_shared['id'].isin(test_id_dict['non_shared_bb']), 'binds'] = 0\n",
    "\n",
    "df_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_shared_bb_1st.csv', index = False)\n",
    "df_non_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_non_shared_bb_1st.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "end",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: end"
     ]
    }
   ],
   "source": [
    "raise Exception('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pseudo labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_labeling(fold_id, df_pseudo):\n",
    "\tprint(f\"======== Running training for fold {fold_id} =============\")\n",
    "\t\n",
    "\tdf_pseudo = df_pseudo.copy()\n",
    "\t\n",
    "\t# load weight\n",
    "\tmodel = BioModule()\n",
    "\tckpt_path = find_latest_ckpt_path(fold_id, paths.MODEL_WEIGHTS_DIR) \n",
    "\tweights = torch.load(ckpt_path)['state_dict']\n",
    "\tmodel.load_state_dict(weights)\n",
    "\t\n",
    "\tpreds_oof = predict_in_batches(model, df_pseudo, \n",
    "\t\t\t\t\t\t\t\t\tdf_test_bb1_rdkit,df_test_bb2_rdkit, df_test_bb3_rdkit, df_test_bb1_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\t\tdf_test_bb1_ecfp4,df_test_bb2_ecfp4, df_test_bb3_ecfp4, df_test_bb1_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\t\tmode='test')\n",
    "\t\n",
    "\tdf_pseudo[TARGETS] = preds_oof\n",
    "\t\n",
    "\tdf_pseudo.to_parquet(paths.OUTPUT_DIR / f\"test_pseudo_label_fold_{fold_id}.parquet\") \n",
    "\t\n",
    "\tdel model, weights, df_pseudo, preds_oof\n",
    "\tgc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_id in [0,1,2,3,4]:\n",
    "\tpseudo_labeling(fold_id, df_pseudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train with Pseudo-label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestの記述子をまとめる\n",
    "df_bb1_rdkit = pd.concat([df_train_bb1_rdkit, df_test_bb1_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb1_ecfp4 = pd.concat([df_train_bb1_ecfp4, df_test_bb1_ecfp4], axis=0).reset_index(drop=True)\n",
    "df_bb2_rdkit = pd.concat([df_train_bb2_rdkit, df_test_bb2_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb2_ecfp4 = pd.concat([df_train_bb2_ecfp4, df_test_bb2_ecfp4], axis=0).reset_index(drop=True)\n",
    "df_bb3_rdkit = pd.concat([df_train_bb3_rdkit, df_test_bb3_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb3_ecfp4 = pd.concat([df_train_bb3_ecfp4, df_test_bb3_ecfp4], axis=0).reset_index(drop=True)\n",
    "df_bb1_scf_rdkit = pd.concat([df_train_bb1_scf_rdkit, df_test_bb1_scf_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb1_scf_ecfp4 = pd.concat([df_train_bb1_scf_ecfp4, df_test_bb1_scf_ecfp4], axis=0).reset_index(drop=True)\n",
    "\n",
    "# train, testを結合した分、testのidxにオフセットを加える\n",
    "bb1_offset = len(df_train_bb1_rdkit)\n",
    "bb2_offset = len(df_train_bb2_rdkit)\n",
    "bb3_offset = len(df_train_bb3_rdkit)\n",
    "bb1_scf_offset = len(df_train_bb1_scf_rdkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioPseudoLabelDataModule(LightningDataModule):\n",
    "\tdef __init__(self, df_train, fold_id):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.train_df = df_train[df_train['fold'] != fold_id]\n",
    "\t\tself.valid_df = df_train[df_train['fold'] == fold_id]\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\ttrain_dataset = BioDataset(self.train_df, \n",
    "\t\t\t\t\t\t\t\t   df_bb1_rdkit,df_bb2_rdkit, df_bb3_rdkit, df_bb1_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\t\tdf_bb1_ecfp4,df_bb2_ecfp4, df_bb3_ecfp4, df_bb1_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\t   mode='train')\n",
    "\t\ttrain_dataloader = torch.utils.data.DataLoader(\n",
    "\t\t\t\t\t\t\t\ttrain_dataset,\n",
    "\t\t\t\t\t\t\t\tbatch_size=config.BATCH_SIZE,\n",
    "\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\tnum_workers=config.NUM_WORKERS,\n",
    "\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\tpersistent_workers=True,\n",
    "\t\t\t\t\t\t\t\tdrop_last=True,\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\treturn train_dataloader\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\tvalid_dataset = BioDataset(self.valid_df, \n",
    "\t\t\t\t\t\t\t\t   df_bb1_rdkit,df_bb2_rdkit, df_bb3_rdkit, df_bb1_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\t\tdf_bb1_ecfp4,df_bb2_ecfp4, df_bb3_ecfp4, df_bb1_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\t   mode='valid')\n",
    "\t\tvalid_dataloader = torch.utils.data.DataLoader(\n",
    "\t\t\t\t\t\t\t\t\t\t\tvalid_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_size=config.BATCH_SIZE,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=config.NUM_WORKERS,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpin_memory=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tpersistent_workers=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdrop_last=False,\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\treturn valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_offset_to_idx(df_pseudo):\n",
    "\t# train_dataの分だけtest datanのidxにオフセットを加える\n",
    "\tdf_pseudo_fold = df_pseudo.copy()\n",
    "\tdf_pseudo_fold['buildingblock1_smiles'] += bb1_offset\n",
    "\tdf_pseudo_fold['buildingblock2_smiles'] += bb2_offset\n",
    "\tdf_pseudo_fold['buildingblock3_smiles'] += bb3_offset\n",
    "\tdf_pseudo_fold['bb1_scaffold_idx'] += bb1_scf_offset\n",
    "\n",
    "\treturn df_pseudo_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_with_pseudolabel(fold_id, df_train):\n",
    "\tprint(f\"======== Running training for fold {fold_id} =============\")\n",
    "\t\n",
    "\t# pseudo_label付テストデータを読み込む\n",
    "\tdf_pseudo_fold = pd.read_parquet(paths.OUTPUT_DIR / f\"test_pseudo_label_fold_{fold_id}.parquet\")\n",
    "\tdf_pseudo_fold = add_offset_to_idx(df_pseudo_fold)\n",
    "\tdf_pseudo_fold['fold'] = -1\n",
    "\t\n",
    "\tdf = pd.concat([df_train, df_pseudo_fold], axis=0).reset_index(drop=True)\n",
    "\t\n",
    "\t# == init data module and model ==\n",
    "\tmodel = BioModule()\n",
    "\tdatamodule = BioPseudoLabelDataModule(df, fold_id)\n",
    "\t\n",
    "\t# == init callback ==\n",
    "\tcheckpoint_callback = ModelCheckpoint(\n",
    "\t\t\t\t\t\t\t\t\t\tmonitor='valid_score',\n",
    "\t\t\t\t\t\t\t\t\t\t  dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "\t\t\t\t\t\t\t\t\t\t  save_top_k=1,\n",
    "\t\t\t\t\t\t\t\t\t\t  save_last=False,\n",
    "\t\t\t\t\t\t\t\t\t\t  save_weights_only=True,\n",
    "\t\t\t\t\t\t\t\t\t\t  filename=f\"fold_{fold_id}_2nd\",\n",
    "\t\t\t\t\t\t\t\t\t\t  mode='max'\n",
    "\t\t\t\t\t\t\t\t\t\t  )\n",
    "\tearly_stop_callback = EarlyStopping(\n",
    "\t\tmonitor='valid_score',\n",
    "\t\tmode=\"max\", \n",
    "\t\tpatience=config.PATIENCE,\n",
    "\t\tverbose=True\n",
    "\t\t)\n",
    "\tcallbacks_to_use = [checkpoint_callback,\n",
    "\t\t\t\t\t\tearly_stop_callback,\n",
    "\t\t\t\t\t\tRichModelSummary(),\n",
    "\t\t\t\t\t\tRichProgressBar(),\n",
    "\t\t\t\t\t   ]\n",
    "\n",
    "\t# == init trainer ==\n",
    "\ttrainer = Trainer(\n",
    "\t\tmax_epochs=config.EPOCHS,\n",
    "\t\tcallbacks=callbacks_to_use,\n",
    "\t\taccelerator=device,\n",
    "\t\tdevices=-1,  # 全ての利用可能なGPUを使用\n",
    "\t\tdeterministic=False,\n",
    "\t\tprecision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "\t\tlogger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}_2nd'),\n",
    "\t)\n",
    "\n",
    "\t# == Training ==\n",
    "\ttrainer.fit(model, datamodule=datamodule)\n",
    "\tweights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "\t\t\n",
    "\tmodel.load_state_dict(weights)\n",
    "\t\n",
    "\tvalid_df = datamodule.valid_df\n",
    "\t\n",
    "\tpreds_oof = predict_in_batches(model, valid_df, \n",
    "\t\t\t\t\t\t\t\t\tdf_bb1_rdkit,df_bb2_rdkit, df_bb3_rdkit, df_bb1_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\t\tdf_bb1_ecfp4,df_bb2_ecfp4, df_bb3_ecfp4, df_bb1_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\t   mode='valid')\n",
    "\ty_oof = valid_df[TARGETS].values\n",
    "\t\n",
    "\tscore_BRD4, score_HSA, score_sEH, score = calc_score(preds_oof, y_oof)\n",
    "\t\n",
    "\tvalid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
    "\t\n",
    "\tprint(f'fold:{fold_id} | CV score = {score}')\n",
    "\t\n",
    "\tdf_test_temp = df_test.drop(['id'], axis=1)\n",
    "\tpreds_test = predict_in_batches(model, df_test_temp, \n",
    "\t\t\t\t\t\t\t\t\t  df_test_bb1_rdkit,df_test_bb2_rdkit, df_test_bb3_rdkit, df_test_bb1_scf_rdkit,\n",
    "\t\t\t\t\t\t\t\t\tdf_test_bb1_ecfp4,df_test_bb2_ecfp4, df_test_bb3_ecfp4, df_test_bb1_scf_ecfp4,\n",
    "\t\t\t\t\t\t\t\t\tmode='test')\n",
    "\t\n",
    "\tdel model, datamodule, trainer, preds_oof, y_oof\n",
    "\tgc.collect()\n",
    "\t\n",
    "\tscore_dict = {\n",
    "\t\t'BRD4':score_BRD4,\n",
    "\t\t\"HSA\":score_HSA,\n",
    "\t\t\"sEH\":score_sEH,\n",
    "\t\t\"all\":score\n",
    "\t}\n",
    "\t\n",
    "\treturn preds_test, score_dict, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "score_list = []\n",
    "score_list_BRD4 = []\n",
    "score_list_HSA = []\n",
    "score_list_sEH = []\n",
    "\n",
    "def save_list_by_text(score_list, filename):\n",
    "\t# ファイルに書き込み\n",
    "\tscore_list_txt = [str(loss) for loss in score_list]\n",
    "\twith open(paths.OUTPUT_DIR / f'{filename}.txt', 'w') as file:\n",
    "\t\tfile.write(', '.join(score_list_txt))\n",
    "\t\n",
    "\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "\t\n",
    "\tpreds_test, score_dict, df_oof = run_training_with_pseudolabel(fold_id, df_train)\n",
    "\t\n",
    "\t# save score\n",
    "\tscore_list_BRD4.append(score_dict['BRD4'])\n",
    "\tscore_list_HSA.append(score_dict['HSA'])\n",
    "\tscore_list_sEH.append(score_dict['sEH'])\n",
    "\tscore_list.append(score_dict['all'])\n",
    "\t\n",
    "\tsave_list_by_text(score_list, 'cv_all_2nd')\n",
    "\tsave_list_by_text(score_list_BRD4, 'cv_BRD4_2nd')\n",
    "\tsave_list_by_text(score_list_HSA, 'cv_HSA_2nd')\n",
    "\tsave_list_by_text(score_list_sEH, 'cv_sEH_2nd')\n",
    "\t\n",
    "\t# save preds（foldごと）\n",
    "\tall_preds.append(preds_test) \n",
    "\t\n",
    "\tdf_oof.to_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}_2nd.parquet\")\n",
    "\t\n",
    "\tdel df_oof\n",
    "\tgc.collect()\n",
    "\t\n",
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "\tdf_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}_2nd.parquet\")\n",
    "\tdf_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "\n",
    "df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(all_preds, 0)\n",
    "\n",
    "df_test['binds'] = 0\n",
    "df_test.loc[df_test['protein_name']=='BRD4', 'binds'] = preds[df_test['protein_name']=='BRD4', 0]\n",
    "df_test.loc[df_test['protein_name']=='HSA', 'binds'] = preds[df_test['protein_name']=='HSA', 1]\n",
    "df_test.loc[df_test['protein_name']=='sEH', 'binds'] = preds[df_test['protein_name']=='sEH', 2]\n",
    "df_test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_2nd.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split sharedbb, nonsharedbb\n",
    "df_sub = pd.read_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_2nd.csv')\n",
    "\n",
    "# load parquet dict data\n",
    "with open(paths.DATA_DIR / 'my-data/test_id_dict.p', 'rb') as file:\n",
    "\ttest_id_dict = pickle.load(file)\n",
    "\t\n",
    "df_shared = df_sub.copy()\n",
    "df_non_shared = df_sub.copy()\n",
    "\n",
    "df_shared.loc[~df_shared['id'].isin(test_id_dict['shared_bb']), 'binds'] = 0\n",
    "df_non_shared.loc[~df_shared['id'].isin(test_id_dict['non_shared_bb']), 'binds'] = 0\n",
    "\n",
    "df_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_shared_bb_2nd.csv', index = False)\n",
    "df_non_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_non_shared_bb_2nd.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8006601,
     "sourceId": 67356,
     "sourceType": "competition"
    },
    {
     "datasetId": 4914065,
     "sourceId": 8275617,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
