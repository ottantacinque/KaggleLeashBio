{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leash Bio\n",
    "\n",
    "- positive data多めに使う\n",
    "- EMAアルゴリズム\n",
    "- EPOCH100, patience10\n",
    "\n",
    "## ref\n",
    "- https://www.kaggle.com/code/yyyu54/pytorch-version-belka-1dcnn-starter-with-all-data\n",
    "- https://www.kaggle.com/code/ahmedelfazouan/belka-1dcnn-starter-with-all-data/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_no = '061'\n",
    "DEBUG = True\n",
    "data_ratio = 1/5\n",
    "\n",
    "infer_only=False\n",
    "fold_list=[0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.12.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install rdkit\n",
    "# !pip install mordred\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# import polars as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score as APS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "# seed_everything\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichModelSummary,\n",
    "    RichProgressBar,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import timm\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "from funcs.utils import find_latest_ckpt_path, del_old_ckpt_path\n",
    "from funcs.calc_descriptor import calc_rdkit_descriptors, calc_ecfp4_descriptors\n",
    "from funcs.tokenize import tokenize_smiles\n",
    "from funcs.tokenize import tokenize_ChemBEATa\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s)\n",
      "pytorch: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def is_kaggle_kernel():\n",
    "    return os.path.exists('/kaggle/working')\n",
    "\n",
    "if is_kaggle_kernel():\n",
    "\n",
    "    BASE_DIR = Path(\"/kaggle\")\n",
    "    DATA_DIR = BASE_DIR / \"input\"\n",
    "    OUTPUT_DIR = BASE_DIR / \"working\"\n",
    "    print('on kaggle notebook')\n",
    "\n",
    "else:\n",
    "    BASE_DIR = Path(os.getcwd()) / './../'\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    OUTPUT_DIR = BASE_DIR / f\"output/exp{exp_no}\"\n",
    "    \n",
    "# set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():    \n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "print('pytorch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    SEED = 2024\n",
    "    \n",
    "    PREPROCESS = False\n",
    "    EPOCHS = 20 #20\n",
    "    PATIENCE = 10 #20\n",
    "    BATCH_SIZE = 4096\n",
    "    NUM_WORKERS = 16\n",
    "    \n",
    "    USE_EMA = False\n",
    "    \n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    MIXED_PRECISION = True\n",
    "    \n",
    "    NUM_FOLDS = 5    \n",
    "    USE_NUM_FOLD = 1\n",
    "    \n",
    "class paths:    \n",
    "    DATA_DIR = DATA_DIR\n",
    "    OUTPUT_DIR = OUTPUT_DIR\n",
    "    MODEL_WEIGHTS_DIR = OUTPUT_DIR / f\"bio-models-exp{exp_no}\"\n",
    "    \n",
    "    SHRUNKEN_DATA_DIR = DATA_DIR / \"shrunken-data-capping\"\n",
    "\n",
    "    TRAIN_PATH = SHRUNKEN_DATA_DIR / \"train.parquet\"\n",
    "    TEST_PATH = SHRUNKEN_DATA_DIR / \"test.parquet\"\n",
    "    SUB_PATH = SHRUNKEN_DATA_DIR / \"sub.parquet\"\n",
    "    \n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    }
   ],
   "source": [
    "print('fix seed')\n",
    "\n",
    "def my_seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "# seed_everything(config.SEED, workers=True)\n",
    "my_seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loda Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_cols = ['buildingblock1_smiles', 'buildingblock2_smiles','buildingblock3_smiles', \n",
    "           'buildingblock1_smiles_scaffold', \"buildingblock2_smiles_scaffold\", \"buildingblock3_smiles_scaffold\",\n",
    "           'fold']\n",
    "\n",
    "TARGETS = ['binds_BRD4', 'binds_HSA','binds_sEH']\n",
    "\n",
    "df_train = pd.read_parquet(paths.TRAIN_PATH, columns=bb_cols + TARGETS)\n",
    "\n",
    "if DEBUG:\n",
    "    df_train = df_train.sample(100000).reset_index(drop=True)\n",
    "else:\n",
    "    # 全てのpositiveサンプルとnegativeサンプルをあわせて、希望の数のdatasetができる様にする\n",
    "    positive = df_train[(df_train[TARGETS]>0).any(axis=1)]\n",
    "    negative = df_train[(df_train[TARGETS]==0).all(axis=1)]\n",
    "\n",
    "    len_train = int(len(df_train)*data_ratio)\n",
    "    use_negative_sample = len_train - len(positive)\n",
    "\n",
    "    df_train = pd.concat([negative.sample(use_negative_sample, random_state=config.SEED), positive],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソフトラベリングを用意する\n",
    "bb1_mean = df_train.groupby('buildingblock1_smiles')[TARGETS].mean()\n",
    "bb2_mean = df_train.groupby('buildingblock2_smiles')[TARGETS].mean()\n",
    "bb3_mean = df_train.groupby('buildingblock3_smiles')[TARGETS].mean()\n",
    "\n",
    "for target in TARGETS:\n",
    "    df_train[f'{target}_bb1'] = df_train['buildingblock1_smiles'].map(bb1_mean[target].to_dict())\n",
    "    df_train[f'{target}_bb2'] = df_train['buildingblock2_smiles'].map(bb2_mean[target].to_dict())\n",
    "    df_train[f'{target}_bb3'] = df_train['buildingblock3_smiles'].map(bb3_mean[target].to_dict())\n",
    "    \n",
    "df_train['binds_BRD4'] = df_train['binds_BRD4'] + df_train['binds_BRD4_bb1'] + df_train['binds_BRD4_bb2'] + df_train['binds_BRD4_bb3']\n",
    "df_train['binds_HSA'] = df_train['binds_HSA'] + df_train['binds_HSA_bb1'] + df_train['binds_HSA_bb2'] + df_train['binds_HSA_bb3']\n",
    "df_train['binds_sEH'] = df_train['binds_sEH'] + df_train['binds_sEH_bb1'] + df_train['binds_sEH_bb2'] + df_train['binds_sEH_bb3']\n",
    "\n",
    "df_train[TARGETS] = df_train[TARGETS].clip(0, 1)\n",
    "\n",
    "df_train.drop(columns=[f'{target}_bb1' for target in TARGETS], inplace=True)\n",
    "df_train.drop(columns=[f'{target}_bb2' for target in TARGETS], inplace=True)\n",
    "df_train.drop(columns=[f'{target}_bb3' for target in TARGETS], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>buildingblock1_smiles_scaffold</th>\n",
       "      <th>buildingblock2_smiles_scaffold</th>\n",
       "      <th>buildingblock3_smiles_scaffold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>91</td>\n",
       "      <td>507</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>160</td>\n",
       "      <td>91</td>\n",
       "      <td>507</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>171</td>\n",
       "      <td>91</td>\n",
       "      <td>507</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>372</td>\n",
       "      <td>91</td>\n",
       "      <td>507</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>561</td>\n",
       "      <td>91</td>\n",
       "      <td>507</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   buildingblock1_smiles  buildingblock2_smiles  buildingblock3_smiles  \\\n",
       "0                      0                     58                     58   \n",
       "1                      0                     58                    160   \n",
       "2                      0                     58                    171   \n",
       "3                      0                     58                    372   \n",
       "4                      0                     58                    561   \n",
       "\n",
       "   buildingblock1_smiles_scaffold  buildingblock2_smiles_scaffold  \\\n",
       "0                              91                             507   \n",
       "1                              91                             507   \n",
       "2                              91                             507   \n",
       "3                              91                             507   \n",
       "4                              91                             507   \n",
       "\n",
       "   buildingblock3_smiles_scaffold  \n",
       "0                             507  \n",
       "1                             776  \n",
       "2                             541  \n",
       "3                             907  \n",
       "4                             543  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submitt用のデータ\n",
    "df_test = pd.read_parquet(paths.SUB_PATH)\n",
    "df_test.head()\n",
    "\n",
    "# preudolabeling用zw\n",
    "cols = ['buildingblock1_smiles', 'buildingblock2_smiles',\n",
    "       'buildingblock3_smiles', 'buildingblock1_smiles_scaffold',\n",
    "       'buildingblock2_smiles_scaffold', 'buildingblock3_smiles_scaffold']\n",
    "df_pseudo = df_test[cols].drop_duplicates().reset_index(drop=True)\n",
    "df_pseudo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変換用辞書を読み込む\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb1_smiles2idx.pickle', mode='rb') as f:\n",
    "    bb1_smiles2idx = pickle.load(f)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb23_smiles2idx.pickle', mode='rb') as f:\n",
    "    bb23_smiles2idx = pickle.load(f)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb1_scaffold_smiles2idx.pickle', mode='rb') as f:\n",
    "    bb1_scaffold_smiles2idx = pickle.load(f)\n",
    "with open(paths.SHRUNKEN_DATA_DIR / 'bb23_scaffold_smiles2idx.pickle', mode='rb') as f:\n",
    "    bb23_scaffold_smiles2idx = pickle.load(f)\n",
    "    \n",
    "bb1_idx2smiles = {v:k for k,v in bb1_smiles2idx.items()}\n",
    "bb23_idx2smiles = {v:k for k,v in bb23_smiles2idx.items()}\n",
    "bb1_scaffold_idx2smiles = {v:k for k,v in bb1_scaffold_smiles2idx.items()}\n",
    "bb23_scaffold_idx2smiles = {v:k for k,v in bb23_scaffold_smiles2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Make Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardization(df_list):\n",
    "    # 複数のdfをまとめて標準化\n",
    "    df_all = pd.concat(df_list,axis=0)\n",
    "    df_all.drop_duplicates(inplace=True)\n",
    "    df_all.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # 標準偏差が0の列を削除\n",
    "    df_all = df_all.loc[:, df_all.std() != 0]\n",
    "\n",
    "    # standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_all)\n",
    "\n",
    "    standardized_df_list = []\n",
    "    for df_temp in df_list:\n",
    "        df_temp = df_temp.loc[:, df_all.columns]\n",
    "        df_temp_std = pd.DataFrame(scaler.transform(df_temp), \n",
    "                                index=df_temp.index, \n",
    "                                columns=df_temp.columns)\n",
    "        standardized_df_list.append(df_temp_std)\n",
    "        \n",
    "    return standardized_df_list\n",
    "\n",
    "\n",
    "def remove_std0(df_list):\n",
    "    # 標準偏差が0の列を削除\n",
    "    df_all = pd.concat(df_list,axis=0)\n",
    "    df_all.drop_duplicates(inplace=True)\n",
    "    df_all = df_all.loc[:, df_all.std() != 0]\n",
    "    \n",
    "    standardized_df_list = []\n",
    "    for df_temp in df_list:\n",
    "        df_temp = df_temp.loc[:, df_all.columns]\n",
    "        standardized_df_list.append(df_temp)\n",
    "        \n",
    "    return standardized_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize smiles\n",
    "df_bb1_token = tokenize_ChemBEATa(bb1_idx2smiles)\n",
    "df_bb23_token = tokenize_ChemBEATa(bb23_idx2smiles)\n",
    "df_bb1_scf_token = tokenize_ChemBEATa(bb1_scaffold_idx2smiles)\n",
    "df_bb23_scf_token = tokenize_ChemBEATa(bb23_scaffold_idx2smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rdkit記述子をまとめて標準化\n",
    "# df_list_rdkit = [\n",
    "#             df_bb1_rdkit,\n",
    "#             df_bb23_rdkit, \n",
    "#             df_bb1_scf_rdkit, \n",
    "#             df_bb23_scf_rdkit,\n",
    "#             ]\n",
    "# df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit = standardization(df_list_rdkit)\n",
    "        \n",
    "# # ECFP4記述子をまとめて標準化\n",
    "# df_list_ecfp4 = [\n",
    "#             df_bb1_ecfp4,\n",
    "#             df_bb23_ecfp4, \n",
    "#             df_bb1_scf_ecfp4, \n",
    "#             df_bb23_scf_ecfp4,\n",
    "#             ]\n",
    "# df_bb1_ecfp4,df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4 = remove_std0(df_list_ecfp4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "# len_rdkit = df_bb1_rdkit.shape[1]\n",
    "# len_ecfp4 = df_bb1_ecfp4.shape[1]\n",
    "len_token = df_bb1_token['input_ids'].shape[1]\n",
    "print(len_token)\n",
    "# print(len_rdkit, len_ecfp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset & DataModule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        bb1_token: dict,\n",
    "        bb23_token: dict,\n",
    "        bb1_scf_token: dict,\n",
    "        bb23_scf_token: dict,\n",
    "        mode = 'train'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert mode in ['train', 'valid', 'test']\n",
    "        self.mode = mode\n",
    "        \n",
    "        meta_cols = [\"buildingblock1_smiles\", # 0\n",
    "                     \"buildingblock2_smiles\", # 1\n",
    "                     \"buildingblock3_smiles\", # 2\n",
    "                     \"buildingblock1_smiles_scaffold\", # 3\n",
    "                     \"buildingblock2_smiles_scaffold\", # 4\n",
    "                     \"buildingblock3_smiles_scaffold\", # 5    \n",
    "                     ]\n",
    "        \n",
    "        if (self.mode == 'train') or (self.mode == 'valid'):\n",
    "            meta_cols += TARGETS\n",
    "            \n",
    "        df = df[meta_cols]\n",
    "        self.df = df[meta_cols].values\n",
    "\n",
    "        self.bb1_input_ids = bb1_token['input_ids']\n",
    "        self.bb23_input_ids = bb23_token['input_ids']\n",
    "        self.bb1_scf_input_ids = bb1_scf_token['input_ids']\n",
    "        self.bb23_scf_input_ids = bb23_scf_token['input_ids']\n",
    "\n",
    "        self.bb1_attn_mask = bb1_token['attention_mask']\n",
    "        self.bb23_attn_mask = bb23_token['attention_mask']\n",
    "        self.bb1_scf_attn_mask = bb1_scf_token['attention_mask']\n",
    "        self.bb23_scf_attn_mask = bb23_scf_token['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.df[index, :]\n",
    "        \n",
    "        bb1_idx = int(row[0])\n",
    "        bb2_idx = int(row[1])\n",
    "        bb3_idx = int(row[2])\n",
    "        bb1_scf_idx = int(row[3])\n",
    "        bb2_scf_idx = int(row[4])\n",
    "        bb3_scf_idx = int(row[5])\n",
    "        \n",
    "        # augmentation\n",
    "        if self.mode == 'train':\n",
    "            if np.random.rand() < 0.5:\n",
    "                bb2_idx, bb3_idx, bb2_scf_idx, bb3_scf_idx = bb3_idx, bb2_idx, bb3_scf_idx, bb2_scf_idx\n",
    "\n",
    "        bb1_input_ids = self.bb1_input_ids[bb1_idx, :]\n",
    "        bb2_input_ids = self.bb23_input_ids[bb2_idx, :]\n",
    "        bb3_input_ids = self.bb23_input_ids[bb3_idx, :]\n",
    "        bb1_scf_input_ids = self.bb1_scf_input_ids[bb1_scf_idx, :]\n",
    "        bb2_scf_input_ids = self.bb23_scf_input_ids[bb2_scf_idx, :]\n",
    "        bb3_scf_input_ids = self.bb23_scf_input_ids[bb3_scf_idx, :]\n",
    "\n",
    "        bb1_attn_mask = self.bb1_attn_mask[bb1_idx, :]\n",
    "        bb2_attn_mask = self.bb23_attn_mask[bb2_idx, :]\n",
    "        bb3_attn_mask = self.bb23_attn_mask[bb3_idx, :]\n",
    "        bb1_scf_attn_mask = self.bb1_scf_attn_mask[bb1_scf_idx, :]\n",
    "        bb2_scf_attn_mask = self.bb23_scf_attn_mask[bb2_scf_idx, :]\n",
    "        bb3_scf_attn_mask = self.bb23_scf_attn_mask[bb3_scf_idx, :]\n",
    "        \n",
    "        # desc1 = np.concatenate([bb1_desc, bb2_desc, bb3_desc, bb1_scf_desc, bb2_scf_desc, bb3_scf_desc])\n",
    "        # desc2 = np.concatenate([bb1_ecfp, bb2_ecfp, bb3_ecfp, bb1_scf_ecfp, bb2_scf_ecfp, bb3_scf_ecfp])\n",
    "        input_ids = np.concatenate([bb2_input_ids, bb2_input_ids, \n",
    "#                                     bb3_input_ids, bb1_scf_input_ids, \n",
    "#                                     bb2_scf_input_ids, bb3_scf_input_ids\n",
    "                                   ])\n",
    "        attn_mask = np.concatenate([bb2_attn_mask, bb2_attn_mask, \n",
    "#                                     bb3_attn_mask, bb1_scf_attn_mask, \n",
    "#                                     bb2_scf_attn_mask, bb3_scf_attn_mask\n",
    "                                   ])\n",
    "                \n",
    "        if (self.mode == 'train') or (self.mode == 'valid'):\n",
    "            y = row[-3:]\n",
    "        else:\n",
    "            y = np.zeros(3)\n",
    "        \n",
    "        output = {\n",
    "            # 'desc1': torch.tensor(desc1, dtype=torch.float32),\n",
    "            # 'desc2': torch.tensor(desc2, dtype=torch.float32),\n",
    "            'X': {'input_ids':input_ids, 'attention_mask':attn_mask},\n",
    "            'y': torch.tensor(y, dtype=torch.float16)\n",
    "        }        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,)\n",
      "(120,)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Check Dataset\n",
    "if DEBUG:\n",
    "    dataset = BioDataset(df_train, \n",
    "                            # df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "                            # df_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "                            df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                            mode='train')\n",
    "    # desc1 = dataset[0]['desc1']\n",
    "    # desc2 = dataset[0]['desc2']\n",
    "    X = dataset[0]['X']\n",
    "    y = dataset[0]['y']\n",
    "    # print(desc1.shape)\n",
    "    # print(desc2.shape)\n",
    "    print(X['input_ids'].shape)\n",
    "    print(X['attention_mask'].shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning data module\n",
    "class BioDataModule(LightningDataModule):\n",
    "    def __init__(self, df_train, fold_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = df_train[df_train['fold'] != fold_id]\n",
    "        self.valid_df = df_train[df_train['fold'] == fold_id]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = BioDataset(self.train_df, \n",
    "                                # df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "                                # df_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "                                 df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                                   mode='train')\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=config.BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=config.NUM_WORKERS,\n",
    "                                pin_memory=True,\n",
    "                                persistent_workers=True,\n",
    "                                drop_last=True,\n",
    "                            )\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = BioDataset(self.valid_df, \n",
    "                                # df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "                                # df_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "                                 df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                                   mode='valid')\n",
    "        valid_dataloader = torch.utils.data.DataLoader(\n",
    "                                            valid_dataset,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config.NUM_WORKERS,\n",
    "                                            pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            drop_last=False,\n",
    "                                        )\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "model_name = \"DeepChem/ChemBERTa-10M-MTR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BioModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name, num_labels=3)\n",
    "        self.chembert = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.config.hidden_size, 3),\n",
    "            # nn.Linear(768, 1024),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(1024, 512),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 3),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 4, 384)\n",
    "        out, hn = self.gru(x)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        \n",
    "        out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n",
    "        out = self.head(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        out = self.chembert(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).last_hidden_state[:, 0]\n",
    "\n",
    "        out = self.head(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model\n",
    "# if DEBUG:\n",
    "#     dummy_model = BioModel()\n",
    "#     total_params = sum(p.numel() for p in dummy_model.parameters())\n",
    "#     print(f\"Total number of parameters: {total_params}\")\n",
    "    \n",
    "#     dataset = BioDataset(df_train, \n",
    "#                             # df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "#                             # df_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "#                             df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "#                             mode='train')\n",
    "#     batch = [dataset[i]['X'] for i in range(8)]\n",
    "\n",
    "#     # token = torch.rand((64, len_token*4), dtype=torch.float32)\n",
    "#     output = dummy_model(batch)\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lightning Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(y_preds, y_true):\n",
    "    \n",
    "    y_true[y_true < 1] = 0\n",
    "    \n",
    "    score_BRD4 = APS(y_true[:,0], y_preds[:,0])\n",
    "    score_HSA = APS(y_true[:,1], y_preds[:,1])\n",
    "    score_sEH = APS(y_true[:,2], y_preds[:,2])\n",
    "    score = (score_BRD4 + score_HSA + score_sEH) / 3\n",
    "    \n",
    "    return score_BRD4, score_HSA, score_sEH, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BioModule, self).__init__()\n",
    "       \n",
    "        self.model = BioModel()\n",
    "        \n",
    "        if config.USE_EMA:\n",
    "            self.ema = ModelEmaV2(self.model, decay=0.999)\n",
    "        \n",
    "        self.validation_step_outputs = []\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, token):\n",
    "        pred = self.model(token)\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-6,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'valid_loss_epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # desc1, desc2, token, y = batch.pop('desc1'), batch.pop('desc2'), batch.pop('token'), batch.pop('y')\n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        train_loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log('train_loss', train_loss,  on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(X))\n",
    "        \n",
    "        # EMAの更新\n",
    "        if config.USE_EMA:\n",
    "            self.ema.update(self.model)\n",
    "        \n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # desc1, desc2, token, y = batch.pop('desc1'), batch.pop('desc2'), batch.pop('token'), batch.pop('y')\n",
    "        X, y = batch.pop('X'), batch.pop('y')\n",
    "        logits = self(X)\n",
    "        preds = torch.sigmoid(logits)\n",
    "        \n",
    "        valid_loss = self.loss_func(logits, y)\n",
    "        \n",
    "        self.log('valid_loss', valid_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True, batch_size=len(X))\n",
    "        \n",
    "        self.validation_step_outputs.append({\"valid_loss\":valid_loss, \"preds\":preds, \"targets\":y})\n",
    "        \n",
    "        return valid_loss\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def calc_score(self, y_preds, y_true):\n",
    "        return calc_score(y_preds, y_true)\n",
    "\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        # 各iterationごとのlossを平均\n",
    "        avg_loss = torch.stack([x['valid_loss'] for x in outputs]).mean()\n",
    "        self.log(\"valid_loss_epoch\", avg_loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        # scoreを計算\n",
    "        y_preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n",
    "        y_true = torch.cat([x['targets'] for x in outputs]).detach().cpu().numpy()\n",
    "        \n",
    "        score = self.calc_score(y_preds, y_true)[-1]\n",
    "        self.log(\"valid_score\", score, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "        \n",
    "        return {'valid_loss_epoch': avg_loss, \"valid_score\":score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, df, \n",
    "                    #    df_bb1_1, df_bb23_1, df_bb1_scf_1, df_bb23_scf_1,\n",
    "                    #    df_bb1_2, df_bb23_2, df_bb1_scf_2, df_bb23_scf_2,\n",
    "                       df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                       mode):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = BioDataset(df, \n",
    "                    #    df_bb1_1, df_bb23_1, df_bb1_scf_1, df_bb23_scf_1,\n",
    "                    #    df_bb1_2, df_bb23_2, df_bb1_scf_2, df_bb23_scf_2,\n",
    "                       df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                         mode=mode)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                                        dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=config.NUM_WORKERS,\n",
    "                                        pin_memory=True,\n",
    "                                        persistent_workers=True,\n",
    "                                        drop_last=False,\n",
    "                                    )\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # desc1 = batch['desc1'].to(device)\n",
    "            # desc2 = batch['desc2'].to(device)\n",
    "            X = batch['X'].to(device)\n",
    "            # logits = model(desc1, desc2, token)\n",
    "            logits = model(X)\n",
    "            preds = torch.sigmoid(logits)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, df):\n",
    "    print(f\"======== Running training for fold {fold_id} =============\")\n",
    "    \n",
    "    # == init data module and model ==\n",
    "    model = BioModule()\n",
    "    datamodule = BioDataModule(df, fold_id)\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                                        monitor='valid_score',\n",
    "                                          dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='max'\n",
    "                                          )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='valid_score',\n",
    "        mode=\"max\", \n",
    "        patience=config.PATIENCE,\n",
    "        verbose=True\n",
    "        )\n",
    "    callbacks_to_use = [checkpoint_callback,\n",
    "                        early_stop_callback,\n",
    "                        RichModelSummary(),\n",
    "                        RichProgressBar(),\n",
    "                       ]\n",
    "\n",
    "    # == init trainer ==\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        callbacks=callbacks_to_use,\n",
    "        accelerator=device,\n",
    "        devices=-1,  # 全ての利用可能なGPUを使用\n",
    "        deterministic=False,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}'),\n",
    "    )\n",
    "    \n",
    "\n",
    "    # == Training ==\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    \n",
    "    del model, datamodule, trainer\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def run_inference(fold_id, df):\n",
    "    print(f\"======== Inference for fold {fold_id} =============\")\n",
    "\n",
    "    # == init data module and model ==\n",
    "    model = BioModule()\n",
    "    datamodule = BioDataModule(df, fold_id)\n",
    "\n",
    "    # infer only\n",
    "    ckpt_path = find_latest_ckpt_path(fold_id, paths.MODEL_WEIGHTS_DIR) \n",
    "    weights = torch.load(ckpt_path)['state_dict']\n",
    "\n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    valid_df = datamodule.valid_df\n",
    "    \n",
    "    preds_oof = predict_in_batches(model, valid_df, \n",
    "                                #   df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "                                # df_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "                                    df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                                mode='valid')\n",
    "    y_oof = valid_df[TARGETS].values\n",
    "    \n",
    "    score_BRD4, score_HSA, score_sEH, score = calc_score(preds_oof, y_oof)\n",
    "    \n",
    "    valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
    "    \n",
    "    print(f'fold:{fold_id} | CV score = {score}')\n",
    "    \n",
    "    df_test_temp = df_test.drop(['id'], axis=1)\n",
    "    preds_test = predict_in_batches(model, df_test_temp, \n",
    "                                # df_bb1_rdkit, df_bb23_rdkit, df_bb1_scf_rdkit, df_bb23_scf_rdkit,\n",
    "                                # df_bb1_ecfp4, df_bb23_ecfp4, df_bb1_scf_ecfp4, df_bb23_scf_ecfp4,\n",
    "                                df_bb1_token, df_bb23_token, df_bb1_scf_token, df_bb23_scf_token,\n",
    "                                mode='test')\n",
    "    \n",
    "    del model, datamodule, preds_oof, y_oof\n",
    "    gc.collect()\n",
    "    \n",
    "    score_dict = {\n",
    "        'BRD4':score_BRD4,\n",
    "        \"HSA\":score_HSA,\n",
    "        \"sEH\":score_sEH,\n",
    "        \"all\":score\n",
    "    }\n",
    "    \n",
    "    return preds_test, score_dict, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/19</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0/19</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:00 • -:--:--</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.00it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 5.000 valid_loss: 0.697      </span>\n",
       "                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">valid_loss_epoch: 0.697 valid_score:</span>\n",
       "                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.005                               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 0/19\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m0/19\u001b[0m \u001b[38;5;245m0:00:00 • -:--:--\u001b[0m \u001b[38;5;249m0.00it/s\u001b[0m \u001b[37mv_num: 5.000 valid_loss: 0.697      \u001b[0m\n",
       "                                                                               \u001b[37mvalid_loss_epoch: 0.697 valid_score:\u001b[0m\n",
       "                                                                               \u001b[37m0.005                               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 23.69 GiB total capacity; 22.67 GiB already allocated; 122.81 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m infer_only:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fold_id \u001b[38;5;129;01min\u001b[39;00m fold_list:\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_id \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[24], line 43\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(fold_id, df)\u001b[0m\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     32\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mEPOCHS,\n\u001b[1;32m     33\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks_to_use,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     logger\u001b[38;5;241m=\u001b[39mTensorBoardLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# == Training ==\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model, datamodule, trainer\n\u001b[1;32m     46\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1030\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    162\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py:1308\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1279\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1283\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/amp.py:77\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[1;32m     80\u001b[0m skip_unscaling \u001b[38;5;241m=\u001b[39m closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 48\u001b[0m, in \u001b[0;36mBioModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     45\u001b[0m     \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# desc1, desc2, token, y = batch.pop('desc1'), batch.pop('desc2'), batch.pop('token'), batch.pop('y')\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m), batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(logits, y)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, train_loss,  on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m, in \u001b[0;36mBioModule.forward\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[0;32m---> 15\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 35\u001b[0m, in \u001b[0;36mBioModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 35\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchembert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(out)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    831\u001b[0m )\n\u001b[0;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         output_attentions,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:410\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:337\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py:226\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    223\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    229\u001b[0m     query_length, key_length \u001b[38;5;241m=\u001b[39m query_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], key_layer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 23.69 GiB total capacity; 22.67 GiB already allocated; 122.81 MiB free; 23.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "all_preds = []\n",
    "score_list = []\n",
    "score_list_BRD4 = []\n",
    "score_list_HSA = []\n",
    "score_list_sEH = []\n",
    "\n",
    "def save_list_by_text(score_list, filename):\n",
    "    # ファイルに書き込み\n",
    "    score_list_txt = [str(loss) for loss in score_list]\n",
    "    with open(paths.OUTPUT_DIR / f'{filename}.txt', 'w') as file:\n",
    "        file.write(', '.join(score_list_txt))\n",
    "\n",
    "# training\n",
    "if not infer_only:\n",
    "    for fold_id in fold_list:\n",
    "        run_training(fold_id, df_train)\n",
    "\n",
    "# inference\n",
    "for fold_id in [0,1,2,3,4]:\n",
    "    preds_test, score_dict, df_oof = run_inference(fold_id, df_train)\n",
    "    \n",
    "    # save score\n",
    "    score_list_BRD4.append(score_dict['BRD4'])\n",
    "    score_list_HSA.append(score_dict['HSA'])\n",
    "    score_list_sEH.append(score_dict['sEH'])\n",
    "    score_list.append(score_dict['all'])\n",
    "    \n",
    "    save_list_by_text(score_list, 'cv_all')\n",
    "    save_list_by_text(score_list_BRD4, 'cv_BRD4')\n",
    "    save_list_by_text(score_list_HSA, 'cv_HSA')\n",
    "    save_list_by_text(score_list_sEH, 'cv_sEH')\n",
    "    \n",
    "    # save preds（foldごと）\n",
    "    all_preds.append(preds_test) \n",
    "    \n",
    "    df_oof.to_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    \n",
    "    del df_oof\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    df_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}.parquet\")\n",
    "    df_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "\n",
    "df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")\n",
    "\n",
    "# 古いckpt pathを削除\n",
    "for fold in range(0, 5): \n",
    "    del_old_ckpt_path(fold, paths.MODEL_WEIGHTS_DIR)\n",
    "    oof_path = paths.OUTPUT_DIR / f'oof_fold_{fold}.parquet'\n",
    "    oof_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(all_preds, 0)\n",
    "\n",
    "df_test['binds'] = 0\n",
    "df_test.loc[df_test['protein_name']=='BRD4', 'binds'] = preds[df_test['protein_name']=='BRD4', 0]\n",
    "df_test.loc[df_test['protein_name']=='HSA', 'binds'] = preds[df_test['protein_name']=='HSA', 1]\n",
    "df_test.loc[df_test['protein_name']=='sEH', 'binds'] = preds[df_test['protein_name']=='sEH', 2]\n",
    "df_test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_1st.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sharedbb, nonsharedbb\n",
    "df_sub = pd.read_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_1st.csv')\n",
    "\n",
    "# load parquet dict data\n",
    "with open(paths.DATA_DIR / 'my-data/test_id_dict.p', 'rb') as file:\n",
    "    test_id_dict = pickle.load(file)\n",
    "    \n",
    "df_shared = df_sub.copy()\n",
    "df_non_shared = df_sub.copy()\n",
    "\n",
    "df_shared.loc[~df_shared['id'].isin(test_id_dict['shared_bb']), 'binds'] = 0\n",
    "df_non_shared.loc[~df_shared['id'].isin(test_id_dict['non_shared_bb']), 'binds'] = 0\n",
    "\n",
    "df_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_shared_bb_1st.csv', index = False)\n",
    "df_non_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_non_shared_bb_1st.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pseudo labeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_labeling(fold_id, df_pseudo):\n",
    "    print(f\"======== Running training for fold {fold_id} =============\")\n",
    "    \n",
    "    df_pseudo = df_pseudo.copy()\n",
    "    \n",
    "    # load weight\n",
    "    model = BioModule()\n",
    "    ckpt_path = find_latest_ckpt_path(fold_id, paths.MODEL_WEIGHTS_DIR) \n",
    "    weights = torch.load(ckpt_path)['state_dict']\n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    preds_oof = predict_in_batches(model, df_pseudo, \n",
    "                                    df_test_bb1_rdkit,df_test_bb2_rdkit, df_test_bb3_rdkit, df_test_bb1_scf_rdkit,\n",
    "                                    df_test_bb1_ecfp4,df_test_bb2_ecfp4, df_test_bb3_ecfp4, df_test_bb1_scf_ecfp4,\n",
    "                                    mode='test')\n",
    "    \n",
    "    df_pseudo[TARGETS] = preds_oof\n",
    "    \n",
    "    df_pseudo.to_parquet(paths.OUTPUT_DIR / f\"test_pseudo_label_fold_{fold_id}.parquet\") \n",
    "    \n",
    "    del model, weights, df_pseudo, preds_oof\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_id in [0,1,2,3,4]:\n",
    "    pseudo_labeling(fold_id, df_pseudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train with Pseudo-label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainとtestの記述子をまとめる\n",
    "df_bb1_rdkit = pd.concat([df_train_bb1_rdkit, df_test_bb1_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb1_ecfp4 = pd.concat([df_train_bb1_ecfp4, df_test_bb1_ecfp4], axis=0).reset_index(drop=True)\n",
    "df_bb2_rdkit = pd.concat([df_train_bb2_rdkit, df_test_bb2_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb2_ecfp4 = pd.concat([df_train_bb2_ecfp4, df_test_bb2_ecfp4], axis=0).reset_index(drop=True)\n",
    "df_bb3_rdkit = pd.concat([df_train_bb3_rdkit, df_test_bb3_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb3_ecfp4 = pd.concat([df_train_bb3_ecfp4, df_test_bb3_ecfp4], axis=0).reset_index(drop=True)\n",
    "df_bb1_scf_rdkit = pd.concat([df_train_bb1_scf_rdkit, df_test_bb1_scf_rdkit], axis=0).reset_index(drop=True)\n",
    "df_bb1_scf_ecfp4 = pd.concat([df_train_bb1_scf_ecfp4, df_test_bb1_scf_ecfp4], axis=0).reset_index(drop=True)\n",
    "\n",
    "# train, testを結合した分、testのidxにオフセットを加える\n",
    "bb1_offset = len(df_train_bb1_rdkit)\n",
    "bb2_offset = len(df_train_bb2_rdkit)\n",
    "bb3_offset = len(df_train_bb3_rdkit)\n",
    "bb1_scf_offset = len(df_train_bb1_scf_rdkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioPseudoLabelDataModule(LightningDataModule):\n",
    "    def __init__(self, df_train, fold_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = df_train[df_train['fold'] != fold_id]\n",
    "        self.valid_df = df_train[df_train['fold'] == fold_id]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = BioDataset(self.train_df, \n",
    "                                   df_bb1_rdkit,df_bb2_rdkit, df_bb3_rdkit, df_bb1_scf_rdkit,\n",
    "                                    df_bb1_ecfp4,df_bb2_ecfp4, df_bb3_ecfp4, df_bb1_scf_ecfp4,\n",
    "                                   mode='train')\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "                                train_dataset,\n",
    "                                batch_size=config.BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                num_workers=config.NUM_WORKERS,\n",
    "                                pin_memory=True,\n",
    "                                persistent_workers=True,\n",
    "                                drop_last=True,\n",
    "                            )\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = BioDataset(self.valid_df, \n",
    "                                   df_bb1_rdkit,df_bb2_rdkit, df_bb3_rdkit, df_bb1_scf_rdkit,\n",
    "                                    df_bb1_ecfp4,df_bb2_ecfp4, df_bb3_ecfp4, df_bb1_scf_ecfp4,\n",
    "                                   mode='valid')\n",
    "        valid_dataloader = torch.utils.data.DataLoader(\n",
    "                                            valid_dataset,\n",
    "                                            batch_size=config.BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config.NUM_WORKERS,\n",
    "                                            pin_memory=True,\n",
    "                                            persistent_workers=True,\n",
    "                                            drop_last=False,\n",
    "                                        )\n",
    "        return valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_offset_to_idx(df_pseudo):\n",
    "    # train_dataの分だけtest datanのidxにオフセットを加える\n",
    "    df_pseudo_fold = df_pseudo.copy()\n",
    "    df_pseudo_fold['buildingblock1_smiles'] += bb1_offset\n",
    "    df_pseudo_fold['buildingblock2_smiles'] += bb2_offset\n",
    "    df_pseudo_fold['buildingblock3_smiles'] += bb3_offset\n",
    "    df_pseudo_fold['bb1_scaffold_idx'] += bb1_scf_offset\n",
    "\n",
    "    return df_pseudo_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_with_pseudolabel(fold_id, df_train):\n",
    "    print(f\"======== Running training for fold {fold_id} =============\")\n",
    "    \n",
    "    # pseudo_label付テストデータを読み込む\n",
    "    df_pseudo_fold = pd.read_parquet(paths.OUTPUT_DIR / f\"test_pseudo_label_fold_{fold_id}.parquet\")\n",
    "    df_pseudo_fold = add_offset_to_idx(df_pseudo_fold)\n",
    "    df_pseudo_fold['fold'] = -1\n",
    "    \n",
    "    df = pd.concat([df_train, df_pseudo_fold], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # == init data module and model ==\n",
    "    model = BioModule()\n",
    "    datamodule = BioPseudoLabelDataModule(df, fold_id)\n",
    "    \n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                                        monitor='valid_score',\n",
    "                                          dirpath=paths.MODEL_WEIGHTS_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=False,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}_2nd\",\n",
    "                                          mode='max'\n",
    "                                          )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='valid_score',\n",
    "        mode=\"max\", \n",
    "        patience=config.PATIENCE,\n",
    "        verbose=True\n",
    "        )\n",
    "    callbacks_to_use = [checkpoint_callback,\n",
    "                        # early_stop_callback,\n",
    "                        RichModelSummary(),\n",
    "                        RichProgressBar(),\n",
    "                       ]\n",
    "\n",
    "    # == init trainer ==\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        callbacks=callbacks_to_use,\n",
    "        accelerator=device,\n",
    "        devices=-1,  # 全ての利用可能なGPUを使用\n",
    "        deterministic=False,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'exp{exp_no}_fold{fold_id}_2nd'),\n",
    "    )\n",
    "\n",
    "    # == Training ==\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    weights = torch.load(checkpoint_callback.best_model_path)['state_dict']\n",
    "        \n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    valid_df = datamodule.valid_df\n",
    "    \n",
    "    preds_oof = predict_in_batches(model, valid_df, \n",
    "                                    df_bb1_rdkit,df_bb2_rdkit, df_bb3_rdkit, df_bb1_scf_rdkit,\n",
    "                                    df_bb1_ecfp4,df_bb2_ecfp4, df_bb3_ecfp4, df_bb1_scf_ecfp4,\n",
    "                                   mode='valid')\n",
    "    y_oof = valid_df[TARGETS].values\n",
    "    \n",
    "    score_BRD4, score_HSA, score_sEH, score = calc_score(preds_oof, y_oof)\n",
    "    \n",
    "    valid_df[[f'{target}_pred' for target in TARGETS]] = preds_oof\n",
    "    \n",
    "    print(f'fold:{fold_id} | CV score = {score}')\n",
    "    \n",
    "    df_test_temp = df_test.drop(['id'], axis=1)\n",
    "    preds_test = predict_in_batches(model, df_test_temp, \n",
    "                                      df_test_bb1_rdkit,df_test_bb2_rdkit, df_test_bb3_rdkit, df_test_bb1_scf_rdkit,\n",
    "                                    df_test_bb1_ecfp4,df_test_bb2_ecfp4, df_test_bb3_ecfp4, df_test_bb1_scf_ecfp4,\n",
    "                                    mode='test')\n",
    "    \n",
    "    del model, datamodule, trainer, preds_oof, y_oof\n",
    "    gc.collect()\n",
    "    \n",
    "    score_dict = {\n",
    "        'BRD4':score_BRD4,\n",
    "        \"HSA\":score_HSA,\n",
    "        \"sEH\":score_sEH,\n",
    "        \"all\":score\n",
    "    }\n",
    "    \n",
    "    return preds_test, score_dict, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "score_list = []\n",
    "score_list_BRD4 = []\n",
    "score_list_HSA = []\n",
    "score_list_sEH = []\n",
    "\n",
    "def save_list_by_text(score_list, filename):\n",
    "    # ファイルに書き込み\n",
    "    score_list_txt = [str(loss) for loss in score_list]\n",
    "    with open(paths.OUTPUT_DIR / f'{filename}.txt', 'w') as file:\n",
    "        file.write(', '.join(score_list_txt))\n",
    "    \n",
    "\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    \n",
    "    preds_test, score_dict, df_oof = run_training_with_pseudolabel(fold_id, df_train)\n",
    "    \n",
    "    # save score\n",
    "    score_list_BRD4.append(score_dict['BRD4'])\n",
    "    score_list_HSA.append(score_dict['HSA'])\n",
    "    score_list_sEH.append(score_dict['sEH'])\n",
    "    score_list.append(score_dict['all'])\n",
    "    \n",
    "    save_list_by_text(score_list, 'cv_all_2nd')\n",
    "    save_list_by_text(score_list_BRD4, 'cv_BRD4_2nd')\n",
    "    save_list_by_text(score_list_HSA, 'cv_HSA_2nd')\n",
    "    save_list_by_text(score_list_sEH, 'cv_sEH_2nd')\n",
    "    \n",
    "    # save preds（foldごと）\n",
    "    all_preds.append(preds_test) \n",
    "    \n",
    "    df_oof.to_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}_2nd.parquet\")\n",
    "    \n",
    "    del df_oof\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "df_oof_all = pd.DataFrame()\n",
    "for fold_id in range(config.NUM_FOLDS):\n",
    "    df_temp = pd.read_parquet(paths.OUTPUT_DIR / f\"oof_fold_{fold_id}_2nd.parquet\")\n",
    "    df_oof_all = pd.concat([df_oof_all, df_temp], axis=0)\n",
    "\n",
    "df_oof_all.to_parquet(paths.OUTPUT_DIR / f\"oof_all.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(all_preds, 0)\n",
    "\n",
    "df_test['binds'] = 0\n",
    "df_test.loc[df_test['protein_name']=='BRD4', 'binds'] = preds[df_test['protein_name']=='BRD4', 0]\n",
    "df_test.loc[df_test['protein_name']=='HSA', 'binds'] = preds[df_test['protein_name']=='HSA', 1]\n",
    "df_test.loc[df_test['protein_name']=='sEH', 'binds'] = preds[df_test['protein_name']=='sEH', 2]\n",
    "df_test[['id', 'binds']].to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_2nd.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split sharedbb, nonsharedbb\n",
    "df_sub = pd.read_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_2nd.csv')\n",
    "\n",
    "# load parquet dict data\n",
    "with open(paths.DATA_DIR / 'my-data/test_id_dict.p', 'rb') as file:\n",
    "    test_id_dict = pickle.load(file)\n",
    "    \n",
    "df_shared = df_sub.copy()\n",
    "df_non_shared = df_sub.copy()\n",
    "\n",
    "df_shared.loc[~df_shared['id'].isin(test_id_dict['shared_bb']), 'binds'] = 0\n",
    "df_non_shared.loc[~df_shared['id'].isin(test_id_dict['non_shared_bb']), 'binds'] = 0\n",
    "\n",
    "df_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_shared_bb_2nd.csv', index = False)\n",
    "df_non_shared.to_csv(paths.OUTPUT_DIR / f'exp{exp_no}_submission_non_shared_bb_2nd.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 古いckpt pathを削除\n",
    "for fold in range(0, 5): \n",
    "    del_old_ckpt_path(fold, paths.MODEL_WEIGHTS_DIR)\n",
    "    \n",
    "    oof_path = paths.OUTPUT_DIR / f'oof_fold_{fold}.parquet'\n",
    "    oof_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8006601,
     "sourceId": 67356,
     "sourceType": "competition"
    },
    {
     "datasetId": 4914065,
     "sourceId": 8275617,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
